{
  "hash": "8bee4f43ccde63945f438a89c2c36550",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: State of the States map analysis\nauthor:\n  - name: Elham Ali\n    corresponding: true\n    roles:\n      - Researcher\n      - Data Storytelling\n      - Human-centered Design\n      - Data Visualization\n    affiliations:\n      - Beeck Center for Social Impact and Innovation at Georgetown University\nlicense: CC BY-SA 4.0\nkeywords:\n  - civic tech\n  - digital transformation\n  - state government\n  - map\n  - research\ndate: last-modified\nabstract: |\n  This analysis aims to explore trends of digital transformation, specifically on policy, chief data officers, digital service teams, impact reports, and design systems across U.S. states and territories.\nkeypoints:\n  - civic tech\n  - digital transformation\n  - state government\n  - map\n  - research\ncitation:\n  container-title: Beeck Center for Social Impact and Innovation\ndraft: false\nbibliography: references.bib\ncode-fold: true\n# reference-location: margin\n# citation-location: margin\necho: true\nwarning: false\n---\n\n## Background\n\nWhen public climate & EJ evidence disappears (removed, restricted, or altered), a decade of downstream knowledge becomes harder to verify, reproduce, teach, or apply—especially for communities and decisions that most need it.\\\nThis analysis looks at how many studies have used these tools, their topics, and their use cases.\n\n## Questions\n\nHere are the key questions explored in this analysis:\n\n-   How many **states/territories** have at least one:\n\n    -   Executive order\n    -   AI Legislation\n    -   Chief Data Officer (CDO)\n    -   Digital Service Team (DST)\n    -   Digital Service Impact Report\n    -   Design system\n\n-   Which states are the **most active vs. least active** across all categories?\n\n-   Which states introduced **CDOs or DSTs earliest**, and how has that spread across regions?\n\n-   Which states with **active CDOs** also have **design systems** or **DSTs**?\n\n-   Are states with **AI legislation** more likely to have other forms of digital infrastructure (DST, design systems)?\n\n-   In which region(s) do **AI legislation policies cluster** in?\n\n-   What are the **most common policy topics** in executive orders (e.g., broadband, interoperability, AI)?\n\n-   What are the **most common policy topics** in AI legislations orders (e.g., broadband, interoperability, AI)?\n\n-   What is the **trend over time** in the number of executive orders and AI legislations enacted?\n\n-   Do **early adopters** (states with policies from 2010–2015) differ from **late adopters** in 2020–2025?\n\n-   Do certain AI legislation topics **cluster in certain regions**? Do certain executive topics topics cluster in certain regions?\n\n## Data Sources\n\nThe climate tools assessed are:\n\n-   map_data\n-   policy_data\n\nThe data for this project comes from the Digital Service Network at the Beeck and last refreshed on September 30, 2025.\n\nOriginal raw datasets are saved in the `data/` folder. This script reduces and cleans those datasets to prepare them for analysis.\n\n------------------------------------------------------------------------\n\n## Cleaning\n\nI start by loading the packages needed for file handling, data wrangling, and visualization.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n## Folder structure helpers\nlibrary(here)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nhere() starts at /Users/elhamali/Documents/Data Projects/state-of-states-map-analysis\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(ezknitr)\n\n## Data import & cleaning\nlibrary(tidyverse)  \n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(janitor)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(lubridate)\nlibrary(janitor)\n# library(rlang)\n\n## Visualization\nlibrary(highcharter)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(igraph)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'igraph'\n\nThe following objects are masked from 'package:lubridate':\n\n    %--%, union\n\nThe following objects are masked from 'package:dplyr':\n\n    as_data_frame, groups, union\n\nThe following objects are masked from 'package:purrr':\n\n    compose, simplify\n\nThe following object is masked from 'package:tidyr':\n\n    crossing\n\nThe following object is masked from 'package:tibble':\n\n    as_data_frame\n\nThe following objects are masked from 'package:stats':\n\n    decompose, spectrum\n\nThe following object is masked from 'package:base':\n\n    union\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(RColorBrewer)\nlibrary(htmlwidgets)\nlibrary(gt)\n```\n:::\n\n\n### Import raw data\n\nI import all .csv files from the `data/` folder, then save them as .rds files into `output/`. This preserves their structure and speeds up future reads.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# List all CSV files\ncsv_files <- list.files(here(\"data\"), pattern = \"\\\\.csv$\", full.names = TRUE)\n\n# Read into a list of dataframes\ndatasets <- map(csv_files, read.csv)\nnames(datasets) <- tools::file_path_sans_ext(basename(csv_files))\n\n# Save each dataset as .rds in output/\nwalk2(\n  datasets,\n  names(datasets),\n  ~ saveRDS(.x, here(\"output\", paste0(.y, \".rds\")))\n)\n```\n:::\n\n\n### Clean both datasets\n\nI apply the same cleaning process to both datasets (map_data and policy_data):\n\n-   Standardize variable names to snake_case\n-   Guess variable types (integers, doubles, dates, etc.)\n-   Convert responses for 'executive_orders', 'ai_legislations', 'digital_service_teams', 'digital_service_impact_reports'\n    -   yes to 1\n    -   no to 0\n-   Convert responses for 'design_systems'\n    -   yes to 1\n    -   in development to 1\n    -   no to 0\n    -   unverified to 0\n-   Convert responses for 'chief_data_officers'\n    -   yes, state CDO to 1\n    -   yes, state CDO equivalent to 1\n    -   no to 0\n    -   no, vacant to 0\n    -   unverified to 0\n-   Add a region variable to classify the state's based on CDC's four regions [^1]\n    -   **Northeast**: Includes Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont\n    -   **Midwest**: Includes Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin\n    -   **South**: Includes Alabama, Arkansas, Delaware, District of Columbia, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, and West Virginia\n    -   **West**: Includes Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.\n    -   **Territories**: Includes American Samoa, Guam, the Northern Mariana Islands, Puerto Rico, and the U.S. Virgin Islands (not included in CDC's regional mapping, but I added here to include the territories)\n\n[^1]: <https://www.cdc.gov/nchs/hus/sources-definitions/geographic-region.htm>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# -------------------------------------------------------------------\n# Libraries\nlibrary(here)\nlibrary(tidyverse)   # includes dplyr, stringr, readr, etc.\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(forcats)\nlibrary(glue)\nlibrary(fs)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'fs'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following object is masked from 'package:igraph':\n\n    path\n```\n\n\n:::\n\n```{.r .cell-code}\n# -------------------------------------------------------------------\n# Helper: read -> clean names -> guess types -> trim\nread_clean_tag <- function(path) {\n  readRDS(path) |>\n    janitor::clean_names() |>\n    readr::type_convert(col_types = readr::cols(.default = readr::col_guess())) |>\n    dplyr::mutate(across(where(is.character), ~ trimws(.x)))\n}\n\n# Core base cleaner (shared)\nclean_base <- function(df) {\n  df |>\n    janitor::clean_names() |>\n    readr::type_convert(col_types = readr::cols(.default = readr::col_guess())) |>\n    dplyr::mutate(across(where(is.character), ~ trimws(.x)))\n}\n\n# -------------------------------------------------------------------\n# Region mapping (CDC 4 regions + Territories) with robust normalization\nadd_region <- function(df, state_col = \"state_territory\") {\n  norm <- function(x) {\n    x |>\n      stringr::str_to_lower() |>\n      stringr::str_replace_all(\"[[:punct:]]\", \" \") |>\n      stringr::str_squish()\n  }\n\n  northeast <- norm(c(\n    \"Connecticut\",\"Maine\",\"Massachusetts\",\"New Hampshire\",\"New Jersey\",\n    \"New York\",\"Pennsylvania\",\"Rhode Island\",\"Vermont\"\n  ))\n  midwest <- norm(c(\n    \"Illinois\",\"Indiana\",\"Iowa\",\"Kansas\",\"Michigan\",\"Minnesota\",\"Missouri\",\n    \"Nebraska\",\"North Dakota\",\"Ohio\",\"South Dakota\",\"Wisconsin\"\n  ))\n  south <- norm(c(\n    \"Alabama\",\"Arkansas\",\"Delaware\",\"District of Columbia\",\"Florida\",\"Georgia\",\n    \"Kentucky\",\"Louisiana\",\"Maryland\",\"Mississippi\",\"North Carolina\",\"Oklahoma\",\n    \"South Carolina\",\"Tennessee\",\"Texas\",\"Virginia\",\"West Virginia\"\n  ))\n  # DC aliases\n  south <- unique(c(south, norm(c(\"Washington, D.C.\",\"Washington D.C.\",\"Washington DC\",\"DC\"))))\n\n  west <- norm(c(\n    \"Alaska\",\"Arizona\",\"California\",\"Colorado\",\"Hawaii\",\"Idaho\",\"Montana\",\"Nevada\",\n    \"New Mexico\",\"Oregon\",\"Utah\",\"Washington\",\"Wyoming\"\n  ))\n  territories <- norm(c(\n    \"American Samoa\",\"Guam\",\"Northern Mariana Islands\",\"Puerto Rico\",\n    \"U.S. Virgin Islands\",\"US Virgin Islands\",\"Virgin Islands\"\n  ))\n\n  # Choose the state column (fallback to 'state' if needed)\n  chosen_col <-\n    if (state_col %in% names(df)) state_col\n    else if (\"state\" %in% names(df)) \"state\"\n    else if (\"state_territory\" %in% names(df)) \"state_territory\"\n    else NA_character_\n\n  if (is.na(chosen_col)) return(df)\n\n  df |>\n    dplyr::mutate(\n      .key = norm(.data[[chosen_col]]),\n      region = dplyr::case_when(\n        .key %in% northeast   ~ \"Northeast\",\n        .key %in% midwest     ~ \"Midwest\",\n        .key %in% south       ~ \"South\",\n        .key %in% west        ~ \"West\",\n        .key %in% territories ~ \"Territories\",\n        TRUE ~ NA_character_\n      )\n    ) |>\n    dplyr::select(-.key)\n}\n\n# -------------------------------------------------------------------\n# Exact recoding rules for specified columns\n\n# yes/no -> 1/0\nrecode_yes_no <- function(x) {\n  xx <- stringr::str_to_lower(stringr::str_squish(as.character(x)))\n  dplyr::case_when(\n    xx == \"yes\" ~ 1L,\n    xx == \"no\"  ~ 0L,\n    TRUE        ~ NA_integer_\n  )\n}\n\n# design_systems: yes/in development -> 1; no/unverified -> 0\nrecode_design_systems <- function(x) {\n  xx <- stringr::str_to_lower(stringr::str_squish(as.character(x)))\n  dplyr::case_when(\n    xx %in% c(\"yes\",\"in development\",\"in-development\",\"in_development\") ~ 1L,\n    xx %in% c(\"no\",\"unverified\") ~ 0L,\n    TRUE ~ NA_integer_\n  )\n}\n\n# chief_data_officers (CDO):\n# yes, state CDO / yes, state CDO equivalent -> 1\n# no / no, vacant / unverified -> 0\nrecode_cdo <- function(x) {\n  xx <- stringr::str_to_lower(stringr::str_squish(as.character(x)))\n  dplyr::case_when(\n    xx %in% c(\"yes, state cdo\",\"yes, state cdo equivalent\") ~ 1L,\n    xx %in% c(\"no\",\"no, vacant\",\"unverified\") ~ 0L,\n    TRUE ~ NA_integer_\n  )\n}\n\n# Apply recoding only to the specified columns if they exist\nrecode_specified_status_cols <- function(df) {\n  df |>\n    dplyr::mutate(\n      dplyr::across(\n        intersect(c(\"executive_orders\", \"ai_legislations\",\n                    \"digital_service_teams\", \"digital_service_impact_reports\"),\n                  names(df)),\n        recode_yes_no\n      ),\n      dplyr::across(\n        intersect(\"design_systems\", names(df)),\n        recode_design_systems\n      ),\n      dplyr::across(\n        intersect(\"chief_data_officers\", names(df)),\n        recode_cdo\n      )\n    )\n}\n\n# -------------------------------------------------------------------\n# One canonical cleaner to use everywhere\nclean_dataset <- function(df) {\n  df |>\n    clean_base() |>\n    recode_specified_status_cols() |>\n    add_region()\n}\n\n# -------------------------------------------------------------------\n# Read raw RDS (from your earlier import step), clean, write outputs\nmap_raw    <- read_clean_tag(here(\"output\", \"map_data.rds\"))\npolicy_raw <- read_clean_tag(here(\"output\", \"policy_data.rds\"))\n\nmap_clean    <- clean_dataset(map_raw)\npolicy_clean <- clean_dataset(policy_raw)\n\nfs::dir_create(here(\"output\"))\nreadr::write_rds(map_clean,    here(\"output\", \"map_data_clean.rds\"))\nreadr::write_rds(policy_clean, here(\"output\", \"policy_data_clean.rds\"))\nreadr::write_csv(map_clean,    here(\"output\", \"map_data_clean.csv\"), na = \"\")\nreadr::write_csv(policy_clean, here(\"output\", \"policy_data_clean.csv\"), na = \"\")\n```\n:::\n\n\nI'll now assign the right variable types for the cleaned datasets.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(readr)\nlibrary(forcats)\n\n# load inputs created earlier\nmap_data_clean    <- readr::read_rds(here(\"output\", \"map_data_clean.rds\"))\npolicy_data_clean <- readr::read_rds(here(\"output\", \"policy_data_clean.rds\"))\n\nto_binary <- function(x) {\n  if (is.logical(x)) return(as.integer(x))\n  if (is.numeric(x)) return(as.integer(x == 1))\n  x_chr <- tolower(trimws(as.character(x)))\n  x_chr[x_chr == \"\"] <- NA\n  yes_vals <- c(\"yes\",\"y\",\"true\",\"t\",\"1\",\"present\",\"has\",\"active\",\"in development\")\n  no_vals  <- c(\"no\",\"n\",\"false\",\"f\",\"0\",\"absent\",\"none\",\"inactive\",\"unverified\",\"no, vacant\")\n  as.integer(dplyr::case_when(\n    x_chr %in% yes_vals ~ 1L,\n    x_chr %in% no_vals  ~ 0L,\n    TRUE ~ NA_integer_\n  ))\n}\n\nto_region_factor <- function(x) {\n  # map region labels to codes 1..5\n  ref <- c(\"Northeast\",\"Midwest\",\"South\",\"West\",\"Territories\")\n  idx <- match(as.character(x), ref)          # NA if label not in ref\n  factor(as.character(idx), levels = as.character(seq_along(ref)), ordered = TRUE)\n}\n\n# ---------- map_data_clean ----------\nmap_int_cols <- c(\n  \"total_number_of_executive_orders\",\n  \"total_number_of_legislation\",\n  \"total_number_of_administrative_rules\",\n  \"publication_year\"\n)\nmap_dbl_cols <- c(\"cdo_year_established\")\nmap_bin_cols <- c(\n  \"executive_orders\",\"ai_legislations\",\"administrative_rules\",\n  \"digital_service_teams\",\"digital_service_impact_reports\",\n  \"chief_data_officers\",\"design_systems\",\"design_system_open_source_status\"\n)\nmap_cat_cols <- c(\"bill_status\")\n\nmap_data_clean <- map_data_clean %>%\n  mutate(across(everything(), ~ as.character(.))) %>%\n  mutate(\n    across(any_of(map_int_cols), ~ suppressWarnings(as.integer(readr::parse_number(.)))),\n    across(any_of(map_dbl_cols), ~ suppressWarnings(as.double(readr::parse_number(.)))),\n    across(any_of(map_bin_cols), to_binary),\n    across(any_of(map_cat_cols), ~ forcats::as_factor(trimws(.))),\n    region = to_region_factor(region)\n  )\n\n# ---------- policy_data_clean ----------\npol_int_cols <- c(\"executive_order_year_enacted\", \"legislative_session\")\npol_cat_cols <- c(\"scan_type\", \"bill_status\")\n\npolicy_data_clean <- policy_data_clean %>%\n  mutate(across(everything(), ~ as.character(.))) %>%\n  mutate(\n    across(any_of(pol_int_cols), ~ suppressWarnings(as.integer(readr::parse_number(.)))),\n    across(any_of(pol_cat_cols), ~ forcats::as_factor(trimws(.))),\n    region = to_region_factor(region)\n  )\n```\n:::\n\n\n## Analysis\n\nI will look at each question one by one and clean the data as I go. I will organize the data during the analysis before exploring the results. I'll also export intermediate results into tidy CSV files so they are ready for further visualization and exploration.\n\n### Q1\n\n-   How many **states/territories** have at least one:\n\n    -   Executive order\n    -   AI Legislation\n    -   Chief Data Officer (CDO)\n    -   Digital Service Team (DST)\n    -   Digital Service Impact Report\n    -   Design system\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nmap_data_clean <- read_rds(here(\"output\", \"map_data_clean.rds\"))\n\n# Packages\nlibrary(tidyverse)\nlibrary(highcharter)\nlibrary(htmlwidgets)\nlibrary(webshot2)\nlibrary(glue)\nlibrary(scales)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'scales'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following object is masked from 'package:purrr':\n\n    discard\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following object is masked from 'package:readr':\n\n    col_factor\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# Indicators to analyze\nindicators <- c(\n  executive_orders                = \"Executive order\",\n  ai_legislations                = \"AI legislation\",\n  chief_data_officers            = \"Chief Data Officer (CDO)\",\n  digital_service_teams          = \"Digital Service Team (DST)\",\n  digital_service_impact_reports = \"Digital Service Impact Report\",\n  design_systems                 = \"Design system\"\n)\n\n# Prep: unique states; turn binary fields into logical presence\n# Works whether columns are 0/1 integers or TRUE/FALSE logicals\nmap_prepped <- map_data_clean %>%\n  distinct(state_territory, .keep_all = TRUE) %>%\n  mutate(\n    across(\n      any_of(names(indicators)),\n      ~ dplyr::coalesce(. == 1L, FALSE)  # TRUE if value == 1; NA -> FALSE\n    )\n  )\n\nn_states <- nrow(map_prepped)\n\n# Summaries\ncounts_tbl <- map_prepped %>%\n  select(any_of(names(indicators))) %>%\n  pivot_longer(everything(), names_to = \"indicator\", values_to = \"has_it\") %>%\n  group_by(indicator) %>%\n  summarise(\n    count   = sum(has_it, na.rm = TRUE),\n    percent = 100 * count / n_states,\n    .groups = \"drop\"\n  ) %>%\n  mutate(label = recode(indicator, !!!indicators))\n\n# Ordered statements\norder_vec <- c(\n  \"executive_orders\",\n  \"ai_legislations\",\n  \"chief_data_officers\",\n  \"design_systems\",\n  \"digital_service_teams\",\n  \"digital_service_impact_reports\"\n)\n\ninsights <- counts_tbl %>%\n  filter(indicator %in% order_vec) %>%\n  mutate(\n    statement = case_when(\n      indicator == \"executive_orders\" ~ glue(\"{number(percent, accuracy = 0.1)}% U.S. states/territories have an executive order.\"),\n      indicator == \"ai_legislations\" ~ glue(\"{number(percent, accuracy = 0.1)}% U.S. states/territories have AI legislation.\"),\n      indicator == \"chief_data_officers\" ~ glue(\"{number(percent, accuracy = 0.1)}% U.S. states/territories have a chief data office (CDO).\"),\n      indicator == \"design_systems\" ~ glue(\"{number(percent, accuracy = 0.1)}% U.S. states/territories have a design system.\"),\n      indicator == \"digital_service_teams\" ~ glue(\"{number(percent, accuracy = 0.1)}% U.S. states/territories have a digital service team (DST).\"),\n      indicator == \"digital_service_impact_reports\" ~ glue(\"{number(percent, accuracy = 0.1)}% U.S. states/territories have a digital service impact report.\")\n    )\n  ) %>%\n  select(indicator, label, count, percent, statement) %>%\n  mutate(percent = round(percent, 1)) %>%\n  arrange(match(indicator, order_vec))\n\n# Save insights\nreadr::write_csv(insights, here(\"output\", \"insights.csv\"))\n\n# Chart\ncounts_for_chart <- counts_tbl %>% arrange(desc(count))\n\nhc <- highchart() %>%\n  hc_chart(type = \"column\") %>%\n  hc_title(text = \"States/Territories with at least one item\") %>%\n  hc_subtitle(text = paste0(\"n = \", n_states, \" states/territories\")) %>%\n  hc_xAxis(categories = counts_for_chart$label, title = list(text = NULL)) %>%\n  hc_yAxis(title = list(text = \"Count\"), allowDecimals = FALSE, min = 0,\n           max = max(counts_for_chart$count)) %>%\n  hc_series(\n    list(\n      name = \"Count\",\n      data = counts_for_chart$count,\n      dataLabels = list(enabled = TRUE),\n      tooltip = list(pointFormat = paste0(\"<b>{point.y}</b> of \", n_states))\n    )\n  ) %>%\n  hc_plotOptions(column = list(borderRadius = 3, pointPadding = 0.05, groupPadding = 0.1)) %>%\n  hc_exporting(enabled = TRUE)\n\nsaveWidget(hc, file = here(\"output\", \"q1.html\"), selfcontained = TRUE)\nwebshot2::webshot(here(\"output\", \"q1.html\"), file = here(\"output\", \"q1.png\"),\n                  vwidth = 900, vheight = 600)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nfile:////Users/elhamali/Documents/Data Projects/state-of-states-map-analysis/output/q1.html screenshot completed\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](analysis_files/figure-html/research-question-1-1.png){width=672}\n:::\n\n```{.r .cell-code .hidden}\ninsights$statement\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n83.9% U.S. states/territories have an executive order.\n55.4% U.S. states/territories have AI legislation.\n51.8% U.S. states/territories have a chief data office (CDO).\n33.9% U.S. states/territories have a design system.\n25.0% U.S. states/territories have a digital service team (DST).\n12.5% U.S. states/territories have a digital service impact report.\n```\n\n\n:::\n:::\n\n\n### Q2\n\n-   Which **regions** have the most vs least:\n\n    -   Executive order\n\n    -   AI Legislations\n\n    -   Chief Data Officers (CDO)\n\n    -   Digital Service Teams (DST)\n\n    -   Digital Service Impact Reports\n\n    -   Design systems\n\nI grouped states and territories into CDC regions and flagged whether each had an executive order, AI legislation, a Chief Data Officer (CDO), a Digital Service Team (DST), a Digital Service Impact Report, or a design system. I then summed counts within each region to identify where adoption was highest and lowest.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(readr); library(here)\nlibrary(dplyr); library(tidyr); library(purrr)\nlibrary(glue)\n\nmap_data_clean <- read_rds(here(\"output\", \"map_data_clean.rds\"))\n\n# Indicators\nindicators <- c(\n  executive_orders                = \"Executive order\",\n  ai_legislations                 = \"AI legislation\",\n  chief_data_officers             = \"Chief Data Officer (CDO)\",\n  digital_service_teams           = \"Digital Service Team (DST)\",\n  digital_service_impact_reports  = \"Digital Service Impact Report\",\n  design_systems                  = \"Design system\"\n)\n\nregion_order <- c(\"Northeast\",\"Midwest\",\"South\",\"West\",\"Territories\")\n\n# Prep: one row per state, coerce to logical\nmap_prepped <- map_data_clean %>%\n  distinct(state_territory, .keep_all = TRUE) %>%\n  filter(!is.na(region)) %>%\n  mutate(region = factor(region, levels = region_order)) %>%\n  mutate(across(any_of(names(indicators)),\n                ~ dplyr::coalesce(. == 1L | . == TRUE, FALSE)))\n\n# Counts by region & indicator\nregion_counts <- map_prepped %>%\n  select(region, any_of(names(indicators))) %>%\n  pivot_longer(-region, names_to = \"indicator\", values_to = \"has_it\") %>%\n  group_by(region, indicator) %>%\n  summarise(count = sum(has_it, na.rm = TRUE), .groups = \"drop\") %>%\n  mutate(label = recode(indicator, !!!indicators)) %>%\n  arrange(label, region)\n\n# Find most/least per indicator\nmost_tbl <- region_counts %>%\n  group_by(indicator) %>%\n  slice_max(count, with_ties = TRUE) %>%\n  ungroup() %>%\n  mutate(which = \"most\")\n\nleast_tbl <- region_counts %>%\n  group_by(indicator) %>%\n  slice_min(count, with_ties = TRUE) %>%\n  ungroup() %>%\n  mutate(which = \"least\")\n\n# Build sentences\npretty_regions <- function(x) paste(x, collapse = \", \")\nmk_sentence <- function(rows, which, obj) {\n  regs <- pretty_regions(rows$region)\n  glue(\"{regs} has the {which} {obj}.\")\n}\n\ninsights_q2 <- tibble(indicator = unique(region_counts$indicator)) %>%\n  mutate(\n    label = recode(indicator, !!!indicators),\n    statement = map_chr(indicator, \\(ind) {\n      most  <- filter(most_tbl,  indicator == ind)\n      least <- filter(least_tbl, indicator == ind)\n      paste(\n        mk_sentence(most, \"most\",  recode(ind, !!!indicators)),\n        mk_sentence(least, \"least\", recode(ind, !!!indicators))\n      )\n    })\n  ) %>%\n  select(label, statement)\n\n# Save both outputs\nwrite_csv(region_counts, here(\"output\", \"region_counts_q2.csv\"))\nwrite_csv(insights_q2,  here(\"output\", \"insights_q2.csv\"))\n\n# Print results in document\n# region_counts\ninsights_q2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 2\n  label                         statement                                       \n  <chr>                         <chr>                                           \n1 AI legislation                South has the most AI legislation. Territories …\n2 Chief Data Officer (CDO)      South, West has the most Chief Data Officer (CD…\n3 Design system                 Northeast, South has the most Design system. Te…\n4 Digital Service Impact Report Northeast has the most Digital Service Impact R…\n5 Digital Service Team (DST)    Northeast has the most Digital Service Team (DS…\n6 Executive order               South has the most Executive order. Territories…\n```\n\n\n:::\n:::\n\n\n### Q3\n\n-   Which states are the **most active vs. least active** across all categories?\n\nTo measure overall state activity, I created a six-item score where states received one point for the presence of each digital activity (executive orders, AI legislation, CDO, DST, Digital Service Impact Report, design system). Each state’s score ranged from 0–6 to indicate the breadth of digital transformation. I then ranked states by total score and identified the most and least active states.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(readr); library(here)\nlibrary(dplyr); library(tidyr); library(purrr); library(glue)\n\n# 1) Load\nmap_data_clean <- read_rds(here(\"output\", \"map_data_clean.rds\"))\n\n# 2) Columns (names = dataset column names; values = friendly labels)\nact_labels <- c(\n  executive_orders               = \"Executive orders\",\n  ai_legislations                = \"AI legislations\",\n  chief_data_officers            = \"Chief Data Officers (CDO)\",\n  digital_service_teams          = \"Digital Service Teams (DST)\",\n  digital_service_impact_reports = \"Digital Service Impact Reports\",\n  design_systems                 = \"Design systems\"\n)\n\n# 3) Presence (0/1) per category, score out of 6 (one row per state)\nstate_counts_q3 <- map_data_clean %>%\n  distinct(state_territory, .keep_all = TRUE) %>%\n  transmute(\n    state_territory,\n    across(\n      all_of(names(act_labels)),\n      ~ as.integer(dplyr::coalesce(. >= 1L, FALSE))\n    )\n  ) %>%\n  mutate(\n    activity_score = rowSums(across(all_of(names(act_labels)))),\n    categories_present = pmap_chr(\n      across(all_of(names(act_labels))),\n      ~ {\n        vals <- c(...)\n        labs <- act_labels[vals == 1]\n        if (length(labs) == 0) \"None\" else paste(labs, collapse = \", \")\n      }\n    )\n  ) %>%\n  arrange(desc(activity_score), state_territory)\n\n# 4) Full ranks (for later viz)\nstate_ranks_q3 <- state_counts_q3 %>%\n  mutate(\n    rank_desc = min_rank(desc(activity_score)),  # 1 = most active\n    rank_asc  = min_rank(activity_score)        # 1 = least active\n  ) %>%\n  arrange(rank_desc, state_territory)\n\n# 5) Top/bottom 10 (ties included at the cutoff)\ntop10 <- state_ranks_q3 %>% filter(rank_desc <= 10)\nbot10 <- state_ranks_q3 %>% filter(rank_asc  <= 10)\n\nfmt_list <- function(df) paste0(df$state_territory, \" (\", df$activity_score, \"/6)\", collapse = \"; \")\n\ninsights_q3 <- tibble(\n  type = c(\"Most active (top 10; ties included)\", \"Least active (bottom 10; ties included)\"),\n  statement = c(fmt_list(top10), fmt_list(bot10))\n)\n\n# 6) Save (match Q2 naming pattern: *_q3.csv) -------------------------------\nif (!dir.exists(here(\"output\"))) dir.create(here(\"output\"), recursive = TRUE)\nwrite_csv(state_counts_q3, here(\"output\", \"state_counts_q3.csv\"))\nwrite_csv(insights_q3,     here(\"output\", \"insights_q3.csv\"))\nwrite_csv(state_ranks_q3,  here(\"output\", \"state_ranks_q3.csv\"))\n\n# 7) Print in document (same pattern as Q2)\n# state_counts_q3\ninsights_q3\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  type                                    statement                             \n  <chr>                                   <chr>                                 \n1 Most active (top 10; ties included)     California (6/6); Colorado (6/6); Mas…\n2 Least active (bottom 10; ties included) Alabama (1/6); Alaska (1/6); American…\n```\n\n\n:::\n:::\n\n\n### Q4\n\n-   Which states introduced **CDOs or DSTs earliest**, and how has that spread across regions?\n\nFor CDOs, I filtered to states with an active CDO and recorded the year the office was established. I then identified the earliest and most recent adopters. For DSTs, I imported a separate dataset of team founding years and names, cleaned it, and identified the earliest and latest established teams.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(readr); library(here)\nlibrary(dplyr); library(glue)\n\n# -------------------------------------------------------------------\n# 1) Load map_data_clean\nmap_data_clean <- read_rds(here(\"output\", \"map_data_clean.rds\"))\n\n# -------------------------------------------------------------------\n# 2) Import and clean dst_data.csv -> dst_data_clean\ndst_data_clean <- read_csv(here(\"data\", \"dst_data.csv\"),\n                           show_col_types = FALSE) %>%\n  mutate(\n    team_year_founded = as.integer(team_year_founded),\n    state_territory   = as.character(state_territory),\n    team_name         = as.character(team_name)\n  )\n\n# Save cleaned DST data\nwrite_rds(dst_data_clean, here(\"output\", \"dst_data_clean.rds\"))\nwrite_csv(dst_data_clean, here(\"output\", \"dst_data_clean.csv\"))\n\n# -------------------------------------------------------------------\n# 3) CDO analysis\ncdo_tbl <- map_data_clean %>%\n  distinct(state_territory, .keep_all = TRUE) %>%\n  filter(!is.na(cdo_year_established) & chief_data_officers >= 1) %>%\n  transmute(state_territory, cdo_year_established)\n\nearliest_cdo <- cdo_tbl %>%\n  filter(cdo_year_established == min(cdo_year_established, na.rm = TRUE))\nlatest_cdo <- cdo_tbl %>%\n  filter(cdo_year_established == max(cdo_year_established, na.rm = TRUE))\n\ninsights_cdo <- tibble(\n  statement = c(\n    glue(\"{earliest_cdo$state_territory} introduced CDOs earliest in {earliest_cdo$cdo_year_established}.\"),\n    glue(\"{latest_cdo$state_territory} introduced CDOs the latest in {latest_cdo$cdo_year_established}.\")\n  )\n)\n\n# -------------------------------------------------------------------\n# 4) DST analysis\ndst_tbl <- dst_data_clean %>%\n  filter(!is.na(team_year_founded)) %>%\n  arrange(team_year_founded)\n\nearliest_dst <- dst_tbl %>%\n  filter(team_year_founded == min(team_year_founded, na.rm = TRUE))\nlatest_dst <- dst_tbl %>%\n  filter(team_year_founded == max(team_year_founded, na.rm = TRUE))\n\ninsights_dst <- tibble(\n  statement = c(\n    glue(\"{earliest_dst$state_territory} introduced its Digital Service Team ({earliest_dst$team_name}) earliest in {earliest_dst$team_year_founded}.\"),\n    glue(\"{latest_dst$state_territory} introduced its Digital Service Team ({latest_dst$team_name}) the latest in {latest_dst$team_year_founded}.\")\n  )\n)\n\n# -------------------------------------------------------------------\n# 5) Save outputs\nif (!dir.exists(here(\"output\"))) dir.create(here(\"output\"), recursive = TRUE)\n\nwrite_csv(cdo_tbl,      here(\"output\", \"cdo_years_q4.csv\"))\nwrite_csv(insights_cdo, here(\"output\", \"insights_cdo_q4.csv\"))\n\nwrite_csv(dst_tbl,      here(\"output\", \"dst_years_q4.csv\"))\nwrite_csv(insights_dst, here(\"output\", \"insights_dst_q4.csv\"))\n\n# -------------------------------------------------------------------\n# 6) Print in document\ncdo_tbl\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        state_territory cdo_year_established\n1               Arizona                 2024\n2              Arkansas                 2017\n3            California                 2019\n4              Colorado                 2010\n5           Connecticut                 2014\n6              Delaware                 2018\n7  District of Columbia                 2017\n8               Florida                 2017\n9                Hawaii                 2022\n10              Indiana                 2017\n11               Kansas                 2024\n12                Maine                 2018\n13             Maryland                 2021\n14        Massachusetts                 2016\n15             Michigan                 2023\n16             Missouri                 2023\n17              Montana                 2021\n18               Nevada                 2024\n19           New Jersey                 2015\n20       North Carolina                 2016\n21         North Dakota                 2018\n22                 Ohio                 2021\n23         Rhode Island                 2024\n24            Tennessee                 2020\n25                Texas                 2015\n26              Vermont                 2017\n27           Washington                 2022\n28              Wyoming                 2019\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\ninsights_cdo\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 1\n  statement                                       \n  <chr>                                           \n1 Colorado introduced CDOs earliest in 2010.      \n2 Arizona introduced CDOs the latest in 2024.     \n3 Kansas introduced CDOs the latest in 2024.      \n4 Nevada introduced CDOs the latest in 2024.      \n5 Rhode Island introduced CDOs the latest in 2024.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\ndst_tbl\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 15 × 6\n   state_territory team_name            team_year_founded team_location_in_org…¹\n   <chr>           <chr>                            <int> <chr>                 \n 1 Delaware        Delaware Government…              2001 Department of State   \n 2 New York        New York State Digi…              2013 Office of General Ser…\n 3 Rhode Island    Rhode Island Enterp…              2013 Department of Adminis…\n 4 North Carolina  North Carolina Digi…              2015 Department of Informa…\n 5 Massachusetts   Massachusetts Digit…              2017 Executive Office of T…\n 6 Vermont         Vermont Agency of D…              2017 Governor's Office     \n 7 New Jersey      New Jersey Office o…              2018 External nonprofit    \n 8 Colorado        Colorado Digital Se…              2019 The Governor's Office…\n 9 Connecticut     Connecticut Digital…              2019 Department of Adminis…\n10 California      California Office o…              2019 Government Operations…\n11 Colorado        Colorado Behavioral…              2022 Behavioral Health Adm…\n12 Pennsylvania    Pennsylvania Common…              2023 Office of Administrat…\n13 Arizona         Arizona Digital Sol…              2024 Arizona Department of…\n14 Maryland        Maryland Digital Se…              2024 Department of Informa…\n15 Minnesota       Minnesota Digital S…              2024 Department of Human S…\n# ℹ abbreviated name: ¹​team_location_in_organization\n# ℹ 2 more variables: operational_status <chr>, funding_model <chr>\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\ninsights_dst\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 1\n  statement                                                                     \n  <chr>                                                                         \n1 Delaware introduced its Digital Service Team (Delaware Government Information…\n2 Arizona introduced its Digital Service Team (Arizona Digital Solutions Office…\n3 Maryland introduced its Digital Service Team (Maryland Digital Service (MDDS)…\n4 Minnesota introduced its Digital Service Team (Minnesota Digital Services, An…\n```\n\n\n:::\n:::\n\n\n### Q5\n\n-   Which states with **active CDOs** also have **design systems** or **DSTs**?\n\nI examined whether states with CDOs also adopted other infrastructure (design systems or DSTs). After filtering to states with active CDOs, I tallied how many also had design systems or DSTs. I then compared the likelihood of having a DST between states with and without CDOs using a relative likelihood ratio.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(readr); library(here)\nlibrary(dplyr); library(glue)\n\n# -------------------------------------------------------------------\n# 1) Load cleaned dataset\nmap_data_clean <- read_rds(here(\"output\", \"map_data_clean.rds\"))\n\n# -------------------------------------------------------------------\n# 2) Prep: keep one row per state/territory, coerce presence to 0/1\nq5_tbl <- map_data_clean %>%\n  distinct(state_territory, .keep_all = TRUE) %>%\n  transmute(\n    state_territory,\n    cdo    = as.integer(coalesce(chief_data_officers,        0L) >= 1L),\n    design = as.integer(coalesce(design_systems,             0L) >= 1L),\n    dst    = as.integer(coalesce(digital_service_teams,      0L) >= 1L)\n  )\n\n# -------------------------------------------------------------------\n# 3) Which states with active CDOs also have design systems or DSTs?\ncdo_active <- q5_tbl %>% filter(cdo == 1)\n\ncdo_with_design_or_dst <- cdo_active %>%\n  filter(design == 1 | dst == 1)\n\nnum_states <- nrow(cdo_with_design_or_dst)\nstate_names <- paste(cdo_with_design_or_dst$state_territory, collapse = \", \")\n\n# -------------------------------------------------------------------\n# 4) Likelihood comparison: probability of DST with CDO vs. without\np_dst_with_cdo <- mean(cdo_active$dst == 1, na.rm = TRUE)\np_dst_without  <- mean(q5_tbl %>% filter(cdo == 0) %>% pull(dst) == 1, na.rm = TRUE)\n\nlikelihood_ratio <- ifelse(p_dst_without > 0, p_dst_with_cdo / p_dst_without, NA_real_)\n\n# Round ratio nicely (e.g., 2.2x more likely)\nratio_text <- ifelse(is.na(likelihood_ratio), \"undefined\",\n                     paste0(round(likelihood_ratio, 1), \"x more\"))\n\n# -------------------------------------------------------------------\n# 5) Build insight statements\ninsights_q5 <- tibble(\n  statement = c(\n    glue(\"{num_states} states with active CDOs also have design systems or DSTs. They are {state_names}.\"),\n    glue(\"States with Chief Data Officers are {ratio_text} likely to have digital service teams.\")\n  )\n)\n\n# -------------------------------------------------------------------\n# 6) Save outputs\nif (!dir.exists(here(\"output\"))) dir.create(here(\"output\"), recursive = TRUE)\nwrite_csv(q5_tbl,        here(\"output\", \"state_counts_q5.csv\"))\nwrite_csv(insights_q5,   here(\"output\", \"insights_q5.csv\"))\n\n# -------------------------------------------------------------------\n# 7) Print results\nq5_tbl\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            state_territory cdo design dst\n1                   Alabama   0      0   0\n2                    Alaska   0      0   0\n3            American Samoa   0      0   0\n4                   Arizona   1      0   1\n5                  Arkansas   1      0   0\n6                California   1      1   1\n7                  Colorado   1      1   1\n8               Connecticut   1      1   1\n9                  Delaware   1      1   1\n10     District of Columbia   1      0   0\n11                  Florida   1      0   0\n12                  Georgia   0      1   0\n13                     Guam   0      0   0\n14                   Hawaii   1      0   0\n15                    Idaho   0      0   0\n16                 Illinois   0      1   0\n17                  Indiana   1      0   0\n18                     Iowa   0      1   0\n19                   Kansas   1      0   0\n20                 Kentucky   0      0   0\n21                Louisiana   0      1   0\n22                    Maine   1      0   0\n23                 Maryland   1      1   1\n24            Massachusetts   1      1   1\n25                 Michigan   1      1   0\n26                Minnesota   0      0   1\n27              Mississippi   0      0   0\n28                 Missouri   1      0   0\n29                  Montana   1      0   0\n30                 Nebraska   1      0   0\n31                   Nevada   1      0   0\n32            New Hampshire   0      0   0\n33               New Jersey   1      1   1\n34               New Mexico   0      0   0\n35                 New York   0      0   1\n36           North Carolina   1      0   1\n37             North Dakota   1      0   0\n38 Northern Mariana Islands   0      0   0\n39                     Ohio   1      1   0\n40                 Oklahoma   0      1   0\n41                   Oregon   0      0   0\n42             Pennsylvania   0      1   1\n43              Puerto Rico   0      0   0\n44             Rhode Island   1      1   1\n45           South Carolina   0      0   0\n46             South Dakota   0      0   0\n47                Tennessee   1      0   0\n48                    Texas   1      0   0\n49      U.S. Virgin Islands   0      0   0\n50                     Utah   0      1   0\n51                  Vermont   1      1   1\n52                 Virginia   0      1   0\n53               Washington   1      0   0\n54            West Virginia   0      0   0\n55                Wisconsin   0      0   0\n56                  Wyoming   1      0   0\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\ninsights_q5\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 1\n  statement                                                                     \n  <chr>                                                                         \n1 13 states with active CDOs also have design systems or DSTs. They are Arizona…\n2 States with Chief Data Officers are 3.4x more likely to have digital service …\n```\n\n\n:::\n:::\n\n\n### Q6\n\n-   Are states with **AI legislation** more likely to have other forms of digital infrastructure (DST, design systems)?\n\nI defined “digital infrastructure” broadly as the presence of any of the following: a CDO, DST, Digital Service Impact Report, or design system. I then compared the probability of having this infrastructure between states with versus without executive orders, AI legislation, or any policy (executive order or AI). Likelihood ratios were calculated to quantify differences, and notable examples were listed.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(readr); library(here)\nlibrary(dplyr); library(glue)\n\n# Load\nmap_data_clean <- read_rds(here(\"output\", \"map_data_clean.rds\"))\n\n# Prep (one row/state; presence 0/1)\nq6_tbl <- map_data_clean %>%\n  distinct(state_territory, .keep_all = TRUE) %>%\n  transmute(\n    state_territory,\n    eo     = as.integer(coalesce(executive_orders,          0L) >= 1L),\n    ai     = as.integer(coalesce(ai_legislations,           0L) >= 1L),\n    cdo    = as.integer(coalesce(chief_data_officers,       0L) >= 1L),\n    dst    = as.integer(coalesce(digital_service_teams,     0L) >= 1L),\n    dsir   = as.integer(coalesce(digital_service_impact_reports, 0L) >= 1L),\n    design = as.integer(coalesce(design_systems,            0L) >= 1L)\n  ) %>%\n  mutate(\n    # DEFINITION NOTE:\n    # \"Digital infrastructure\" = present if ANY of the following are present:\n    #   CDO OR Digital Service Team (DST) OR Digital Service Impact Report (DSIR) OR Design system\n    infra = as.integer(cdo == 1 | dst == 1 | dsir == 1 | design == 1),\n    any_policy = as.integer(eo == 1 | ai == 1)\n  )\n\n# Helper: ratio + notable examples\ncalc_ratio <- function(df, policy_col) {\n  with_policy    <- df %>% filter(.data[[policy_col]] == 1)\n  without_policy <- df %>% filter(.data[[policy_col]] == 0)\n\n  p_with    <- mean(with_policy$infra == 1, na.rm = TRUE)\n  p_without <- mean(without_policy$infra == 1, na.rm = TRUE)\n\n  ratio <- ifelse(p_without > 0, p_with / p_without, NA_real_)\n  ratio_text <- ifelse(is.na(ratio), \"undefined\", paste0(round(ratio, 1), \"x more\"))\n\n  notable <- with_policy %>% filter(infra == 1) %>% pull(state_territory)\n  notable_text <- ifelse(length(notable) > 0, paste(notable, collapse = \", \"), \"None\")\n\n  list(ratio_text = ratio_text, notable_text = notable_text)\n}\n\n# Ratios + statements\neo_stats  <- calc_ratio(q6_tbl, \"eo\")\nai_stats  <- calc_ratio(q6_tbl, \"ai\")\nany_stats <- calc_ratio(q6_tbl, \"any_policy\")\n\ninsights_q6 <- tibble(\n  statement = c(\n    glue(\"States with executive orders are {eo_stats$ratio_text} likely to have other forms of digital infrastructure (CDO, DST, Digital Service Impact Report, design system). Notable examples are {eo_stats$notable_text}.\"),\n    glue(\"States with AI legislation are {ai_stats$ratio_text} likely to have other forms of digital infrastructure (CDO, DST, Digital Service Impact Report, design system). Notable examples are {ai_stats$notable_text}.\"),\n    glue(\"States with any policy (AI legislation and/or executive orders) are {any_stats$ratio_text} likely to have other forms of digital infrastructure (CDO, DST, Digital Service Impact Report, design system). Notable examples are {any_stats$notable_text}.\")\n  )\n)\n\n# Save outputs\nif (!dir.exists(here(\"output\"))) dir.create(here(\"output\"), recursive = TRUE)\nwrite_csv(q6_tbl,      here(\"output\", \"state_counts_q6.csv\"))\nwrite_csv(insights_q6, here(\"output\", \"insights_q6.csv\"))\n\n# Print results\nq6_tbl\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            state_territory eo ai cdo dst dsir design infra any_policy\n1                   Alabama  1  0   0   0    0      0     0          1\n2                    Alaska  1  0   0   0    0      0     0          1\n3            American Samoa  1  0   0   0    0      0     0          1\n4                   Arizona  0  1   1   1    0      0     1          1\n5                  Arkansas  0  1   1   0    0      0     1          1\n6                California  1  1   1   1    1      1     1          1\n7                  Colorado  1  1   1   1    1      1     1          1\n8               Connecticut  1  1   1   1    0      1     1          1\n9                  Delaware  1  1   1   1    0      1     1          1\n10     District of Columbia  1  0   1   0    0      0     1          1\n11                  Florida  1  1   1   0    0      0     1          1\n12                  Georgia  1  1   0   0    0      1     1          1\n13                     Guam  1  0   0   0    0      0     0          1\n14                   Hawaii  1  1   1   0    0      0     1          1\n15                    Idaho  1  1   0   0    0      0     0          1\n16                 Illinois  1  1   0   0    0      1     1          1\n17                  Indiana  1  1   1   0    0      0     1          1\n18                     Iowa  1  0   0   0    0      1     1          1\n19                   Kansas  1  0   1   0    0      0     1          1\n20                 Kentucky  1  0   0   0    0      0     0          1\n21                Louisiana  1  0   0   0    0      1     1          1\n22                    Maine  0  1   1   0    0      0     1          1\n23                 Maryland  1  1   1   1    0      1     1          1\n24            Massachusetts  1  1   1   1    1      1     1          1\n25                 Michigan  1  1   1   0    0      1     1          1\n26                Minnesota  1  1   0   1    1      0     1          1\n27              Mississippi  1  0   0   0    0      0     0          1\n28                 Missouri  0  0   1   0    0      0     1          0\n29                  Montana  1  0   1   0    0      0     1          1\n30                 Nebraska  0  0   1   0    0      0     1          0\n31                   Nevada  1  0   1   0    0      0     1          1\n32            New Hampshire  1  1   0   0    0      0     0          1\n33               New Jersey  1  1   1   1    1      1     1          1\n34               New Mexico  1  0   0   0    0      0     0          1\n35                 New York  1  1   0   1    1      0     1          1\n36           North Carolina  1  1   1   1    1      0     1          1\n37             North Dakota  1  1   1   0    0      0     1          1\n38 Northern Mariana Islands  1  0   0   0    0      0     0          1\n39                     Ohio  1  1   1   0    0      1     1          1\n40                 Oklahoma  1  0   0   0    0      1     1          1\n41                   Oregon  1  0   0   0    0      0     0          1\n42             Pennsylvania  1  0   0   1    0      1     1          1\n43              Puerto Rico  0  0   0   0    0      0     0          0\n44             Rhode Island  1  0   1   1    0      1     1          1\n45           South Carolina  1  0   0   0    0      0     0          1\n46             South Dakota  0  0   0   0    0      0     0          0\n47                Tennessee  0  1   1   0    0      0     1          1\n48                    Texas  1  1   1   0    0      0     1          1\n49      U.S. Virgin Islands  0  1   0   0    0      0     0          1\n50                     Utah  1  1   0   0    0      1     1          1\n51                  Vermont  1  1   1   1    0      1     1          1\n52                 Virginia  1  1   0   0    0      1     1          1\n53               Washington  1  1   1   0    0      0     1          1\n54            West Virginia  1  1   0   0    0      0     0          1\n55                Wisconsin  1  0   0   0    0      0     0          1\n56                  Wyoming  1  0   1   0    0      0     1          1\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\ninsights_q6\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 1\n  statement                                                                     \n  <chr>                                                                         \n1 States with executive orders are 1.1x more likely to have other forms of digi…\n2 States with AI legislation are 1.8x more likely to have other forms of digita…\n3 States with any policy (AI legislation and/or executive orders) are 1.4x more…\n```\n\n\n:::\n:::\n\n\n### Q7\n\n-   In which region(s) do **AI legislation policies cluster** in?\n\nTo assess whether AI legislation clusters in particular regions, I calculated both **absolute counts** and **relative proportions**. First, I assigned each state or territory to its CDC region grouping and flagged whether it had AI legislation. I then aggregated by region to count the number of states with AI legislation (absolute clustering) and the percentage of states within the region that had AI legislation (relative clustering). Absolute counts highlight where most AI laws are located overall, while proportions account for differences in region size.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(readr); library(here)\nlibrary(dplyr); library(glue); library(tidyr)\n\n# 1) Load\nmap_data_clean <- read_rds(here(\"output\", \"map_data_clean.rds\"))\n\n# 2) Prep: one row per state/territory with region + AI presence (0/1)\nq7_tbl <- map_data_clean %>%\n  distinct(state_territory, .keep_all = TRUE) %>%\n  transmute(\n    state_territory,\n    region = as.character(region),\n    ai_legislation = as.integer(dplyr::coalesce(ai_legislations, 0L) >= 1L)\n  )\n\n# 3) Summarize by region: counts and proportions\nregion_ai_counts <- q7_tbl %>%\n  group_by(region) %>%\n  summarise(\n    n_states       = n(),\n    ai_states      = sum(ai_legislation, na.rm = TRUE),\n    percent_with_ai= round(100 * ai_states / n_states, 1),\n    .groups = \"drop\"\n  ) %>%\n  arrange(desc(ai_states), desc(percent_with_ai))\n\n# 4) Identify clustering by absolute count and by proportion (tie-aware)\nmax_count <- max(region_ai_counts$ai_states, na.rm = TRUE)\nmax_prop  <- max(region_ai_counts$percent_with_ai, na.rm = TRUE)\n\ncluster_count_regions <- region_ai_counts %>%\n  filter(ai_states == max_count) %>%\n  pull(region)\n\ncluster_prop_regions <- region_ai_counts %>%\n  filter(percent_with_ai == max_prop) %>%\n  pull(region)\n\nfmt_regions <- function(x) paste(x, collapse = \", \")\n\n# 5) Insight statements (both views)\ninsights_q7 <- tibble(\n  statement = c(\n    glue(\"By absolute count, AI legislation tends to cluster in the {fmt_regions(cluster_count_regions)}.\"),\n    glue(\"By share of states, AI legislation tends to cluster in the {fmt_regions(cluster_prop_regions)}.\")\n  )\n)\n\n# 6) Save\nif (!dir.exists(here(\"output\"))) dir.create(here(\"output\"), recursive = TRUE)\nwrite_csv(region_ai_counts, here(\"output\", \"region_ai_counts_q7.csv\"))\nwrite_csv(insights_q7,      here(\"output\", \"insights_q7.csv\"))\n\n# 7) Print\nregion_ai_counts\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 4\n  region      n_states ai_states percent_with_ai\n  <chr>          <int>     <int>           <dbl>\n1 South             17        10            58.8\n2 Northeast          9         7            77.8\n3 West              13         7            53.8\n4 Midwest           12         6            50  \n5 Territories        5         1            20  \n```\n\n\n:::\n\n```{.r .cell-code .hidden}\ninsights_q7\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 1\n  statement                                                            \n  <chr>                                                                \n1 By absolute count, AI legislation tends to cluster in the South.     \n2 By share of states, AI legislation tends to cluster in the Northeast.\n```\n\n\n:::\n:::\n\n\n### Q8\n\n-   What are the **most common policy topics** in executive orders (e.g., broadband, interoperability, AI)?\n\nI parsed `executive_order_last_topic_tag_s` by normalizing separators (commas/semicolons → semicolons), splitting multi-topic strings into individual rows, trimming whitespace, and standardizing to title case. I then counted topic frequencies across all executive orders to identify the most and least common topics. For the word cloud, I used the `wordcloud` package with a fixed seed for reproducibility, scaling word size by frequency and using a categorical palette (no stemming or stop-word removal, because these are curated tags rather than free text).\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(readr); library(here)\nlibrary(dplyr); library(tidyr); library(stringr); library(glue)\nlibrary(wordcloud); library(RColorBrewer)\n\n# 1) Load\npolicy_data_clean <- read_rds(here(\"output\", \"policy_data_clean.rds\"))\n\n# 2) Extract + clean EO topics (split multi-tags, normalize separators/spaces/case)\neo_topics <- policy_data_clean %>%\n  filter(!is.na(executive_order_last_topic_tag_s)) %>%\n  mutate(topic_str = str_replace_all(executive_order_last_topic_tag_s, \"[,|;]\", \";\")) %>%\n  separate_rows(topic_str, sep = \";\") %>%\n  mutate(topic = str_squish(str_to_title(topic_str))) %>%\n  filter(topic != \"\")\n\n# 3) Count topic frequency\ntopic_counts <- eo_topics %>%\n  count(topic, sort = TRUE, name = \"count\")\n\n# 4) Top/Bottom 10 (ties kept)\ntop10_topics    <- topic_counts %>% slice_max(count, n = 10, with_ties = TRUE)\nbottom10_topics <- topic_counts %>% slice_min(count, n = 10, with_ties = TRUE)\n\ninsights_q8 <- tibble(\n  statement = c(\n    glue(\"The 10 most common policy topics in executive orders are: {paste(top10_topics$topic, collapse = ', ')}.\"),\n    glue(\"The 10 least common policy topics in executive orders are: {paste(bottom10_topics$topic, collapse = ', ')}.\")\n  )\n)\n\n# 5) Save tables\nif (!dir.exists(here(\"output\"))) dir.create(here(\"output\"), recursive = TRUE)\nwrite_csv(topic_counts,    here(\"output\", \"eo_topic_counts_q8.csv\"))\nwrite_csv(insights_q8,     here(\"output\", \"insights_q8.csv\"))\nwrite_csv(top10_topics,    here(\"output\", \"eo_top10_topics_q8.csv\"))\nwrite_csv(bottom10_topics, here(\"output\", \"eo_bottom10_topics_q8.csv\"))\n\n# 6) Word cloud PNG (deterministic)\nset.seed(123)\npng(filename = here(\"output\", \"eo_topics_wordcloud_q8.png\"),\n    width = 1400, height = 1000, res = 150, bg = \"white\")\nsuppressWarnings(\n  wordcloud(words = topic_counts$topic,\n            freq  = topic_counts$count,\n            min.freq = 1,\n            max.words = 200,\n            random.order = FALSE,\n            rot.per = 0.10,\n            scale = c(4, 0.8),\n            colors = brewer.pal(8, \"Dark2\"))\n)\ndev.off()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nquartz_off_screen \n                2 \n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# 7) Print in document\ntopic_counts\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 36 × 2\n   topic                         count\n   <chr>                         <int>\n 1 Cybersecurity                    52\n 2 Information Technology           45\n 3 Broadband                        38\n 4 Administrative Reorganization    29\n 5 Interoperability                 29\n 6 Website/Portal                   28\n 7 Technology Modernization         17\n 8 Data Sharing                     14\n 9 Procurement                      13\n10 Public Safety                    13\n# ℹ 26 more rows\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\ninsights_q8\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 1\n  statement                                                                     \n  <chr>                                                                         \n1 The 10 most common policy topics in executive orders are: Cybersecurity, Info…\n2 The 10 least common policy topics in executive orders are: Agile/Lean Operati…\n```\n\n\n:::\n:::\n\n\n### Q9\n\n-   What are the **most common policy topics** in AI legislations orders (e.g., broadband, interoperability, AI)?\n\nI analyzed policy topics in AI legislations using the `bill_topic_tag_s_text` field. I normalized multi-topic strings (commas/semicolons → semicolons), split into individual tags, trimmed for whitespace, and standardized to title case. Because every law in this dataset is inherently about artificial intelligence, I excluded the term *“Artificial Intelligence”* to avoid inflating its frequency. Topics were then tallied across all AI legislations to identify the most and least common. For visualization, I generated a word cloud with word size scaled by topic frequency and colors drawn from a categorical palette. A fixed random seed ensures reproducibility.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(readr); library(here)\nlibrary(dplyr); library(tidyr); library(stringr); library(glue)\nlibrary(wordcloud); library(RColorBrewer)\n\n# 1) Load\npolicy_data_clean <- read_rds(here(\"output\", \"policy_data_clean.rds\"))\n\n# 2) Extract + clean AI legislation topics\n# Variable of interest: bill_topic_tag_s_text (multi-value tags)\nai_topics <- policy_data_clean %>%\n  filter(!is.na(bill_topic_tag_s_text)) %>%\n  mutate(topic_str = str_replace_all(bill_topic_tag_s_text, \"[,|;]\", \";\")) %>%\n  separate_rows(topic_str, sep = \";\") %>%\n  mutate(topic = str_squish(str_to_title(topic_str))) %>%\n  filter(topic != \"\") %>%\n  filter(topic != \"Artificial Intelligence\")   # remove base term to avoid inflation\n\n# 3) Count frequency\nai_topic_counts <- ai_topics %>%\n  count(topic, sort = TRUE, name = \"count\")\n\n# 4) Top/Bottom 10 (ties kept)\ntop10_ai_topics    <- ai_topic_counts %>% slice_max(count, n = 10, with_ties = TRUE)\nbottom10_ai_topics <- ai_topic_counts %>% slice_min(count, n = 10, with_ties = TRUE)\n\ninsights_q9 <- tibble(\n  statement = c(\n    glue(\"The 10 most common policy topics in AI legislations are: {paste(top10_ai_topics$topic, collapse = ', ')}.\"),\n    glue(\"The 10 least common policy topics in AI legislations are: {paste(bottom10_ai_topics$topic, collapse = ', ')}.\")\n  )\n)\n\n# 5) Save tables\nif (!dir.exists(here(\"output\"))) dir.create(here(\"output\"), recursive = TRUE)\nwrite_csv(ai_topic_counts,    here(\"output\", \"ai_topic_counts_q9.csv\"))\nwrite_csv(insights_q9,        here(\"output\", \"insights_q9.csv\"))\nwrite_csv(top10_ai_topics,    here(\"output\", \"ai_top10_topics_q9.csv\"))\nwrite_csv(bottom10_ai_topics, here(\"output\", \"ai_bottom10_topics_q9.csv\"))\n\n# 6) Word cloud PNG (deterministic)\nset.seed(123)\npng(filename = here(\"output\", \"ai_topics_wordcloud_q9.png\"),\n    width = 1400, height = 1000, res = 150, bg = \"white\")\nsuppressWarnings(\n  wordcloud(words = ai_topic_counts$topic,\n            freq  = ai_topic_counts$count,\n            min.freq = 1,\n            max.words = 200,\n            random.order = FALSE,\n            rot.per = 0.10,\n            scale = c(4, 0.8),\n            colors = brewer.pal(8, \"Dark2\"))\n)\ndev.off()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nquartz_off_screen \n                2 \n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# 7) Print in document\nai_topic_counts\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 21 × 2\n   topic                 count\n   <chr>                 <int>\n 1 Criminal/Legal System    11\n 2 Education                10\n 3 Civil Rights              8\n 4 Public Health             8\n 5 Public Safety             8\n 6 Voting/Elections          3\n 7 Blockchain                2\n 8 Data Governance           2\n 9 Data Privacy              2\n10 Emergency Management      2\n# ℹ 11 more rows\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\ninsights_q9\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 1\n  statement                                                                     \n  <chr>                                                                         \n1 The 10 most common policy topics in AI legislations are: Criminal/Legal Syste…\n2 The 10 least common policy topics in AI legislations are: Administrative Reor…\n```\n\n\n:::\n:::\n\n\n### Q10\n\n-   What is the **trend over time** in the number of executive orders and AI legislations enacted?\n\nI examined temporal trends by counting the number of executive orders and AI legislations enacted each year. For executive orders, I used the `executive_order_year_enacted` variable; for AI legislations, I used the `legislative_session` variable. Counts were aggregated by year, with missing years filled as zeros to provide a continuous timeline. I then described whether activity has generally increased, decreased, or remained the same by looking at the overall slope of that line (positive/negative/flat) for the overall pattern of counts across time.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(readr); library(here)\nlibrary(dplyr); library(tidyr); library(stringr); library(glue)\n\n# 1) Load\npolicy_data_clean <- read_rds(here(\"output\", \"policy_data_clean.rds\"))\n\n# 2) Normalize scan_type and build yearly counts for both series\ndat <- policy_data_clean %>%\n  mutate(scan_type_norm = str_to_lower(str_squish(scan_type)))\n\neo_by_year <- dat %>%\n  filter(str_detect(scan_type_norm, \"executive\"),\n         !is.na(executive_order_year_enacted)) %>%\n  transmute(year = as.integer(executive_order_year_enacted)) %>%\n  count(year, name = \"count\") %>%\n  mutate(type = \"Executive orders\")\n\nai_by_year <- dat %>%\n  filter(str_detect(scan_type_norm, \"legis\"),\n         !is.na(legislative_session)) %>%\n  transmute(year = as.integer(legislative_session)) %>%\n  count(year, name = \"count\") %>%\n  mutate(type = \"AI legislations\")\n\n# 3) Fill missing years with zero so both series span the same axis\nall_years <- sort(unique(c(eo_by_year$year, ai_by_year$year)))\ntrend_wide <- tibble(year = all_years) %>%\n  left_join(select(eo_by_year, year, eo = count), by = \"year\") %>%\n  left_join(select(ai_by_year, year, ai = count), by = \"year\") %>%\n  mutate(across(c(eo, ai), ~ replace_na(., 0L)))\n\ntrend_long <- trend_wide %>%\n  pivot_longer(c(eo, ai), names_to = \"series\", values_to = \"count\") %>%\n  mutate(type = recode(series, eo = \"Executive orders\", ai = \"AI legislations\")) %>%\n  select(year, type, count) %>%\n  arrange(year, type)\n\n# 4) Direction statements via simple linear trend on filled series\nslope_dir <- function(x_year, y_count) {\n  if (length(unique(x_year)) < 2) return(\"remained the same\")\n  s <- coef(lm(y_count ~ x_year))[2]\n  if (is.na(s) || abs(s) < 1e-9) \"remained the same\" else if (s > 0) \"increased\" else \"decreased\"\n}\neo_dir <- slope_dir(trend_wide$year, trend_wide$eo)\nai_dir <- slope_dir(trend_wide$year, trend_wide$ai)\n\ninsights_q10 <- tibble(\n  statement = c(\n    glue(\"On average, the number of executive orders around digital transformation in government have {eo_dir} over the years.\"),\n    glue(\"On average, the number of AI legislations around digital transformation in government have {ai_dir} over the years.\")\n  )\n)\n\n# 5) Save + print\nif (!dir.exists(here(\"output\"))) dir.create(here(\"output\"), recursive = TRUE)\nwrite_csv(trend_long,   here(\"output\", \"trend_data_q10.csv\"))\nwrite_csv(insights_q10, here(\"output\", \"insights_q10.csv\"))\n\ntrend_long\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 26 × 3\n    year type             count\n   <int> <chr>            <int>\n 1  2013 AI legislations      0\n 2  2013 Executive orders    26\n 3  2014 AI legislations      0\n 4  2014 Executive orders    22\n 5  2015 AI legislations      0\n 6  2015 Executive orders    34\n 7  2016 AI legislations      0\n 8  2016 Executive orders    24\n 9  2017 AI legislations      0\n10  2017 Executive orders    34\n# ℹ 16 more rows\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\ninsights_q10\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 1\n  statement                                                                     \n  <chr>                                                                         \n1 On average, the number of executive orders around digital transformation in g…\n2 On average, the number of AI legislations around digital transformation in go…\n```\n\n\n:::\n:::\n\n\n### Q11\n\n-   Do certain AI legislation topics **cluster in certain regions**? Do certain executive topics topics cluster in certain regions?\n\nFor executive orders, I used the `executive_order_last_topic_tag_s` variable, and for AI legislation, I used the `bill_topic_tag_s_text` variable. Multi-topic strings were split on commas and semicolons, trimmed, and normalized. For AI legislation, I removed the generic tag “artificial intelligence” since the dataset itself is scoped to AI-related laws, and including it would artificially inflate its frequency. I then counted topic mentions by region and reported the top five most common executive order topics and top five AI legislation topics for each region , with word size proportional to topic frequency. The results are reported as paired statements by region.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(readr); library(here)\nlibrary(dplyr); library(tidyr); library(stringr); library(glue)\nlibrary(purrr)\nlibrary(wordcloud); library(RColorBrewer)\n\n# 1) Load\npolicy_data_clean <- read_rds(here(\"output\", \"policy_data_clean.rds\"))\n\n# 2) Helper: safe filename\nslug <- function(x) {\n  x |> tolower() |> str_replace_all(\"[^a-z0-9]+\", \"_\") |> str_replace_all(\"^_|_$\", \"\")\n}\n\n# 3) Executive order topics by region (split inline; no map/list-cols)\neo_topics_by_region <- policy_data_clean %>%\n  filter(!is.na(region), !is.na(executive_order_last_topic_tag_s)) %>%\n  transmute(region,\n            topic = executive_order_last_topic_tag_s) %>%\n  separate_rows(topic, sep = \"[,;]\") %>%\n  mutate(topic = str_squish(str_to_title(topic))) %>%\n  filter(topic != \"\") %>%\n  count(region, topic, sort = TRUE)\n\n# 4) AI legislation topics by region (exclude \"Artificial Intelligence\")\nai_topics_by_region <- policy_data_clean %>%\n  filter(!is.na(region), !is.na(bill_topic_tag_s_text)) %>%\n  transmute(region,\n            topic = bill_topic_tag_s_text) %>%\n  separate_rows(topic, sep = \"[,;]\") %>%\n  mutate(topic = str_squish(str_to_title(topic))) %>%\n  filter(topic != \"\", topic != \"Artificial Intelligence\") %>%\n  count(region, topic, sort = TRUE)\n\n# 5) Top 5 per region (EO & AI)\neo_top5 <- eo_topics_by_region %>%\n  group_by(region) %>%\n  slice_max(n, n = 5, with_ties = FALSE) %>%\n  summarise(eo_top5 = paste(topic, collapse = \", \"), .groups = \"drop\")\n\nai_top5 <- ai_topics_by_region %>%\n  group_by(region) %>%\n  slice_max(n, n = 5, with_ties = FALSE) %>%\n  summarise(ai_top5 = paste(topic, collapse = \", \"), .groups = \"drop\")\n\nregion_topics_q11 <- eo_top5 %>%\n  full_join(ai_top5, by = \"region\") %>%\n  arrange(region)\n\n# 6) Statements (one per region, exactly as requested)\ninsights_q11 <- region_topics_q11 %>%\n  mutate(statement = glue(\n    \"The top five most common executive order topics in the {region} are {eo_top5}, \",\n    \"while {ai_top5} are the top five most common.\"\n  )) %>%\n  select(region, statement)\n\n# 7) Save tables\nif (!dir.exists(here(\"output\"))) dir.create(here(\"output\"), recursive = TRUE)\nwrite_csv(eo_topics_by_region, here(\"output\", \"eo_topics_by_region_q11.csv\"))\nwrite_csv(ai_topics_by_region, here(\"output\", \"ai_topics_by_region_q11.csv\"))\nwrite_csv(region_topics_q11,   here(\"output\", \"region_topics_q11.csv\"))\nwrite_csv(insights_q11,        here(\"output\", \"insights_q11.csv\"))\n\n# 8) Regional word clouds (EO and AI); deterministic with set.seed\nset.seed(123)\nunique_regions <- sort(unique(c(eo_topics_by_region$region, ai_topics_by_region$region)))\n\nwalk(unique_regions, function(reg) {\n  # EO word cloud for region (if data exists)\n  eo_reg <- eo_topics_by_region %>% filter(region == reg)\n  if (nrow(eo_reg) > 0) {\n    png(filename = here(\"output\", paste0(\"wordcloud_eo_\", slug(reg), \"_q11.png\")),\n        width = 1400, height = 1000, res = 150, bg = \"white\")\n    suppressWarnings(\n      wordcloud(words = eo_reg$topic,\n                freq  = eo_reg$n,\n                min.freq = 1,\n                max.words = 200,\n                random.order = FALSE,\n                rot.per = 0.10,\n                scale = c(4, 0.8),\n                colors = brewer.pal(8, \"Dark2\"))\n    )\n    dev.off()\n  }\n\n  # AI word cloud for region (if data exists)\n  ai_reg <- ai_topics_by_region %>% filter(region == reg)\n  if (nrow(ai_reg) > 0) {\n    png(filename = here(\"output\", paste0(\"wordcloud_ai_\", slug(reg), \"_q11.png\")),\n        width = 1400, height = 1000, res = 150, bg = \"white\")\n    suppressWarnings(\n      wordcloud(words = ai_reg$topic,\n                freq  = ai_reg$n,\n                min.freq = 1,\n                max.words = 200,\n                random.order = FALSE,\n                rot.per = 0.10,\n                scale = c(4, 0.8),\n                colors = brewer.pal(8, \"Dark2\"))\n    )\n    dev.off()\n  }\n})\n\n# 9) Print in document\nregion_topics_q11\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 3\n  region      eo_top5                                                    ai_top5\n  <chr>       <chr>                                                      <chr>  \n1 Midwest     Website/Portal, Information Technology, Broadband, Cybers… Public…\n2 Northeast   Administrative Reorganization, Information Technology, Cy… Civil …\n3 South       Cybersecurity, Information Technology, Broadband, Adminis… Educat…\n4 Territories Administrative Reorganization, Broadband, Information Tec… Crimin…\n5 West        Broadband, Interoperability, Cybersecurity, Information T… Crimin…\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\ninsights_q11\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 2\n  region      statement                                                         \n  <chr>       <glue>                                                            \n1 Midwest     The top five most common executive order topics in the Midwest ar…\n2 Northeast   The top five most common executive order topics in the Northeast …\n3 South       The top five most common executive order topics in the South are …\n4 Territories The top five most common executive order topics in the Territorie…\n5 West        The top five most common executive order topics in the West are B…\n```\n\n\n:::\n:::\n\n\n### Q12\n\n-   How do states that first adopted **(early adopters)** digital government executive order policies between **2013–2015** differ from those **(late adopters)** that first adopted them between **2023–2025** in terms of:\n\n    -   Policy topics (executive order topic tags)\n\n    -   Regional distribution (region 1–5 categories)\n\nI compared early adopters of digital government executive orders (2013–2015) to late adopters (2023–2025). First, I grouped states into early or late cohorts using the `executive_order_year_enacted` field. Multi-topic strings in `executive_order_last_topic_tag_s` were split on commas/semicolons, trimmed, and standardized to title case. I summarized the top five topics for each group and visualized them with word clouds. Regional distributions were assessed by counting the number of executive orders per region for early vs. late adopters. Given the limited sample sizes and topic sparsity, I focused on descriptive comparisons only rather than formal statistical testing.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(readr); library(here)\nlibrary(dplyr); library(tidyr); library(stringr); library(glue)\nlibrary(wordcloud); library(RColorBrewer)\n\n# 1) Load data\npolicy_data_clean <- read_rds(here(\"output\", \"policy_data_clean.rds\"))\n\n# 2) Subset executive orders with years\neo <- policy_data_clean %>%\n  filter(!is.na(executive_order_year_enacted),\n         !is.na(executive_order_last_topic_tag_s),\n         executive_order_year_enacted >= 2013,\n         executive_order_year_enacted <= 2025) %>%\n  select(state_territory, region, executive_order_year_enacted,\n         executive_order_last_topic_tag_s)\n\n# 3) Group into early vs late adopters\neo <- eo %>%\n  mutate(period = case_when(\n    between(executive_order_year_enacted, 2013, 2015) ~ \"Early (2013–2015)\",\n    between(executive_order_year_enacted, 2023, 2025) ~ \"Late (2023–2025)\",\n    TRUE ~ NA_character_\n  )) %>%\n  filter(!is.na(period))\n\n# 4) Split topics\neo_topics <- eo %>%\n  separate_rows(executive_order_last_topic_tag_s, sep = \"[,;]\") %>%\n  mutate(topic = str_squish(str_to_title(executive_order_last_topic_tag_s))) %>%\n  filter(topic != \"\")\n\n# 5) Frequency tables\ntopic_counts <- eo_topics %>%\n  count(period, topic, sort = TRUE)\n\ntop5_early <- topic_counts %>%\n  filter(period == \"Early (2013–2015)\") %>%\n  slice_max(n, n = 5, with_ties = FALSE)\n\ntop5_late <- topic_counts %>%\n  filter(period == \"Late (2023–2025)\") %>%\n  slice_max(n, n = 5, with_ties = FALSE)\n\n# 6) Regional distribution (counts per region)\nregion_counts_q12 <- eo %>%\n  count(period, region, name = \"count\")\n\n# 7) Statements (purely descriptive)\ninsights_q12 <- tibble(\n  statement = c(\n    glue(\"The top five most common executive order topics among early adopters (2013–2015) were {paste(top5_early$topic, collapse = ', ')}, while late adopters (2023–2025) focused more on {paste(top5_late$topic, collapse = ', ')}.\"),\n    glue(\"Regionally, early adopters were concentrated in: {paste(unique(region_counts_q12$region[region_counts_q12$period == 'Early (2013–2015)']), collapse = ', ')}; late adopters in: {paste(unique(region_counts_q12$region[region_counts_q12$period == 'Late (2023–2025)']), collapse = ', ')}.\")\n  )\n)\n\n# 8) Save outputs\nif (!dir.exists(here(\"output\"))) dir.create(here(\"output\"), recursive = TRUE)\nwrite_csv(topic_counts,      here(\"output\", \"eo_topic_counts_q12.csv\"))\nwrite_csv(region_counts_q12, here(\"output\", \"region_counts_q12.csv\"))\nwrite_csv(insights_q12,      here(\"output\", \"insights_q12.csv\"))\n\n# 9) Word clouds (early vs late)\nset.seed(123)\nfor (grp in unique(eo_topics$period)) {\n  dat <- eo_topics %>% filter(period == grp) %>% count(topic)\n  if (nrow(dat) > 0) {\n    png(filename = here(\"output\", paste0(\"wordcloud_\", str_replace_all(grp, \"[^A-Za-z0-9]\", \"_\"), \"_q12.png\")),\n        width = 1400, height = 1000, res = 150, bg = \"white\")\n    wordcloud(words = dat$topic,\n              freq = dat$n,\n              min.freq = 1,\n              max.words = 200,\n              random.order = FALSE,\n              rot.per = 0.10,\n              scale = c(4, 0.8),\n              colors = brewer.pal(8, \"Dark2\"))\n    dev.off()\n  }\n}\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nWarning in wordcloud(words = dat$topic, freq = dat$n, min.freq = 1, max.words =\n200, : Information Technology could not be fit on page. It will not be plotted.\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# 10) Print\ntop5_early\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 3\n  period            topic                      n\n  <chr>             <chr>                  <int>\n1 Early (2013–2015) Cybersecurity             18\n2 Early (2013–2015) Information Technology    18\n3 Early (2013–2015) Interoperability          17\n4 Early (2013–2015) Website/Portal            11\n5 Early (2013–2015) Broadband                  9\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\ntop5_late\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 3\n  period           topic                             n\n  <chr>            <chr>                         <int>\n1 Late (2023–2025) Artificial Intelligence          10\n2 Late (2023–2025) Administrative Reorganization     4\n3 Late (2023–2025) Broadband                         4\n4 Late (2023–2025) Digital Service Team              4\n5 Late (2023–2025) Case/Application Processing       2\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nregion_counts_q12\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              period      region count\n1  Early (2013–2015)     Midwest    10\n2  Early (2013–2015)   Northeast    24\n3  Early (2013–2015)       South    26\n4  Early (2013–2015) Territories     4\n5  Early (2013–2015)        West    18\n6   Late (2023–2025)     Midwest     2\n7   Late (2023–2025)   Northeast    10\n8   Late (2023–2025)       South     6\n9   Late (2023–2025) Territories     4\n10  Late (2023–2025)        West     6\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\ninsights_q12\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 1\n  statement                                                                     \n  <chr>                                                                         \n1 The top five most common executive order topics among early adopters (2013–20…\n2 Regionally, early adopters were concentrated in: Midwest, Northeast, South, T…\n```\n\n\n:::\n:::\n\n\n### Q13\n\n-   How do states that first adopted **(early adopters)** AI legislation between **2019–2021** differ from those **(late adopters)** that first adopted it between **2023–2025** in terms of:\n\n    -   Policy topics (bill topic tags, excluding the baseline “artificial intelligence” tag)\n\n    -   Regional distribution (region 1–5 categories)\n\nI compared early adopters of AI legislation (2019–2021) to late adopters (2023–2025). I grouped states by adoption period using the `legislative_session` field. Policy topics (`bill_topic_tag_s_text`) were split on commas/semicolons, cleaned, and standardized to title case. Because all bills in this dataset are inherently AI-related, I excluded the generic tag *“Artificial Intelligence”* to avoid inflating its frequency and to highlight the other substantive policy areas addressed in AI legislation (e.g., data governance, privacy, workforce). I then summarized the top five topics for each group, created word clouds for visualization, and compared regional distributions by counting the number of adopting states in each CDC region.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(readr); library(here)\nlibrary(dplyr); library(tidyr); library(stringr); library(glue)\nlibrary(wordcloud); library(RColorBrewer)\n\n# 1) Load data\npolicy_data_clean <- read_rds(here(\"output\", \"policy_data_clean.rds\"))\n\n# 2) Subset AI legislations with years\nai <- policy_data_clean %>%\n  filter(!is.na(legislative_session),\n         !is.na(bill_topic_tag_s_text),\n         legislative_session >= 2019,\n         legislative_session <= 2025) %>%\n  select(state_territory, region, legislative_session,\n         bill_topic_tag_s_text)\n\n# 3) Group into early vs late adopters\nai <- ai %>%\n  mutate(period = case_when(\n    between(legislative_session, 2019, 2021) ~ \"Early (2019–2021)\",\n    between(legislative_session, 2023, 2025) ~ \"Late (2023–2025)\",\n    TRUE ~ NA_character_\n  )) %>%\n  filter(!is.na(period))\n\n# 4) Split topics, clean, and exclude \"Artificial Intelligence\"\nai_topics <- ai %>%\n  separate_rows(bill_topic_tag_s_text, sep = \"[,;]\") %>%\n  mutate(topic = str_squish(str_to_title(bill_topic_tag_s_text))) %>%\n  filter(topic != \"\", topic != \"Artificial Intelligence\")\n\n# 5) Frequency tables\ntopic_counts <- ai_topics %>%\n  count(period, topic, sort = TRUE)\n\ntop5_early <- topic_counts %>%\n  filter(period == \"Early (2019–2021)\") %>%\n  slice_max(n, n = 5, with_ties = FALSE)\n\ntop5_late <- topic_counts %>%\n  filter(period == \"Late (2023–2025)\") %>%\n  slice_max(n, n = 5, with_ties = FALSE)\n\n# 6) Regional distribution (counts per region)\nregion_counts_q13 <- ai %>%\n  count(period, region, name = \"count\")\n\n# 7) Statements\ninsights_q13 <- tibble(\n  statement = c(\n    glue(\"Early adopters of AI legislation (2019–2021) were concentrated in regions {paste(unique(region_counts_q13$region[region_counts_q13$period == 'Early (2019–2021)']), collapse = ', ')}, while late adopters (2023–2025) were more evenly spread across regions {paste(unique(region_counts_q13$region[region_counts_q13$period == 'Late (2023–2025)']), collapse = ', ')}.\"),\n    glue(\"The top AI legislation topics for early adopters were {paste(top5_early$topic, collapse = ', ')}, while late adopters more commonly emphasized {paste(top5_late$topic, collapse = ', ')}.\")\n  )\n)\n\n# 8) Save outputs\nif (!dir.exists(here(\"output\"))) dir.create(here(\"output\"), recursive = TRUE)\nwrite_csv(topic_counts,      here(\"output\", \"ai_topic_counts_q13.csv\"))\nwrite_csv(region_counts_q13, here(\"output\", \"region_counts_q13.csv\"))\nwrite_csv(insights_q13,      here(\"output\", \"insights_q13.csv\"))\n\n# 9) Word clouds (early vs late)\nset.seed(123)\nfor (grp in unique(ai_topics$period)) {\n  dat <- ai_topics %>% filter(period == grp) %>% count(topic)\n  if (nrow(dat) > 0) {\n    png(filename = here(\"output\", paste0(\"wordcloud_\", str_replace_all(grp, \"[^A-Za-z0-9]\", \"_\"), \"_q13.png\")),\n        width = 1400, height = 1000, res = 150, bg = \"white\")\n    wordcloud(words = dat$topic,\n              freq = dat$n,\n              min.freq = 1,\n              max.words = 200,\n              random.order = FALSE,\n              rot.per = 0.10,\n              scale = c(4, 0.8),\n              colors = brewer.pal(8, \"Dark2\"))\n    dev.off()\n  }\n}\n\n# 10) Print\ntop5_early\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 3\n  period            topic                     n\n  <chr>             <chr>                 <int>\n1 Early (2019–2021) Public Safety             4\n2 Early (2019–2021) Criminal/Legal System     3\n3 Early (2019–2021) Public Health             3\n4 Early (2019–2021) Blockchain                1\n5 Early (2019–2021) Civil Rights              1\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\ntop5_late\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 3\n  period           topic                     n\n  <chr>            <chr>                 <int>\n1 Late (2023–2025) Education                10\n2 Late (2023–2025) Criminal/Legal System     8\n3 Late (2023–2025) Civil Rights              6\n4 Late (2023–2025) Public Health             5\n5 Late (2023–2025) Public Safety             4\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nregion_counts_q13\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             period      region count\n1 Early (2019–2021)     Midwest     2\n2 Early (2019–2021)   Northeast     2\n3 Early (2019–2021)       South     5\n4 Early (2019–2021)        West     4\n5  Late (2023–2025)     Midwest     6\n6  Late (2023–2025)   Northeast     7\n7  Late (2023–2025)       South    20\n8  Late (2023–2025) Territories     1\n9  Late (2023–2025)        West     9\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\ninsights_q13\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 1\n  statement                                                                     \n  <chr>                                                                         \n1 Early adopters of AI legislation (2019–2021) were concentrated in regions Mid…\n2 The top AI legislation topics for early adopters were Public Safety, Criminal…\n```\n\n\n:::\n:::\n\n\n## Export all insights\n\nI'm almost done! I now merge all insights/statements into one document to use for our reporting.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(tidyverse)\nlibrary(here)\nlibrary(fs)\n\n# 1) Files to include (ordered) — keep Q4 split files; exclude insights_q4.csv\ninsight_files <- c(\n  \"insights.csv\",                # Q1\n  \"insights_q2.csv\",\n  \"insights_q3.csv\",\n  \"insights_cdo_q4.csv\",         # Q4 (part 1)\n  \"insights_dst_q4.csv\",         # Q4 (part 2)\n  \"insights_q5.csv\",\n  \"insights_q6.csv\",\n  \"insights_q7.csv\",\n  \"insights_q8.csv\",\n  \"insights_q9.csv\",\n  \"insights_q10.csv\",\n  \"insights_q11.csv\",\n  \"insights_q12.csv\",\n  \"insights_q13.csv\"\n)\n\n# 2) Map filenames -> question numbers\nq_map <- c(\n  \"insights.csv\"       = \"Q1\",\n  \"insights_q2.csv\"    = \"Q2\",\n  \"insights_q3.csv\"    = \"Q3\",\n  \"insights_cdo_q4.csv\"= \"Q4\",\n  \"insights_dst_q4.csv\"= \"Q4\",\n  \"insights_q5.csv\"    = \"Q5\",\n  \"insights_q6.csv\"    = \"Q6\",\n  \"insights_q7.csv\"    = \"Q7\",\n  \"insights_q8.csv\"    = \"Q8\",\n  \"insights_q9.csv\"    = \"Q9\",\n  \"insights_q10.csv\"   = \"Q10\",\n  \"insights_q11.csv\"   = \"Q11\",\n  \"insights_q12.csv\"   = \"Q12\",\n  \"insights_q13.csv\"   = \"Q13\"\n)\n\n# 3) Map question numbers -> full research question text\nq_text_map <- c(\n  Q1  = \"How many states/territories have at least one: Executive order, AI Legislation, Chief Data Officer (CDO), Digital Service Team (DST), Digital Service Impact Report, Design system\",\n  Q2  = \"Which regions have the most vs least: Executive order, AI Legislations, Chief Data Officers (CDO), Digital Service Teams (DST), Digital Service Impact Reports, Design systems\",\n  Q3  = \"Which states are the most active vs. least active across all categories?\",\n  Q4  = \"Which states introduced CDOs or DSTs earliest, and how has that spread across regions?\",\n  Q5  = \"Which states with active CDOs also have design systems or DSTs?\",\n  Q6  = \"Are states with AI legislation more likely to have other forms of digital infrastructure (DST, design systems)?\",\n  Q7  = \"In which region(s) do AI legislation policies cluster in?\",\n  Q8  = \"What are the most common policy topics in executive orders (e.g., broadband, interoperability, AI)?\",\n  Q9  = \"What are the most common policy topics in AI legislations orders (e.g., broadband, interoperability, AI)?\",\n  Q10 = \"What is the trend over time in the number of executive orders and AI legislations enacted?\",\n  Q11 = \"Do certain AI legislation topics cluster in certain regions? Do certain executive topics topics cluster in certain regions?\",\n  Q12 = \"How do states that first adopted (early adopters) digital government executive order policies between 2013–2015 differ from those (late adopters) that first adopted them between 2023–2025 in terms of: Policy topics (executive order topic tags) and Regional distribution (region 1–5 categories)\",\n  Q13 = \"How do states that first adopted (early adopters) AI legislation between 2019–2021 differ from those (late adopters) that first adopted it between 2023–2025 in terms of: Policy topics (bill topic tags, excluding the baseline 'artificial intelligence' tag) and Regional distribution (region 1–5 categories)\"\n)\n\n# 4) Keep only files that actually exist in output/\ninsight_files <- keep(insight_files, ~ file_exists(here(\"output\", .x)))\n\n# 5) Reader with special handling for Q3 (\"type\" + \"statement\")\nsafe_read <- function(fname) {\n  path <- here(\"output\", fname)\n  df <- readr::read_csv(path, show_col_types = FALSE)\n\n  if (\"statement\" %in% names(df)) {\n    if (\"type\" %in% names(df)) {\n      df <- df %>%\n        mutate(statement = paste0(type, \": \", statement)) %>%\n        select(statement)\n    } else {\n      df <- df %>% select(statement)\n    }\n  } else {\n    return(tibble())\n  }\n\n  qnum <- q_map[[fname]]\n  df %>%\n    mutate(\n      source_file = fname,\n      question_number = qnum,\n      question_text = q_text_map[[qnum]]\n    ) %>%\n    select(question_number, question_text, source_file, statement)\n}\n\n# 6) Bind, keep input order\nall_insights_statements <- map_dfr(insight_files, safe_read) %>%\n  mutate(\n    question_number = factor(\n      question_number,\n      levels = unique(q_map[insight_files])  # preserve order\n    )\n  ) %>%\n  arrange(question_number)\n\n# 7) Save\nreadr::write_csv(all_insights_statements, here(\"output\", \"all_insights_statements.csv\"))\n```\n:::\n\n",
    "supporting": [
      "analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "16faad8badebaaa5e8f125a71a515aef",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: State of the States map analysis\nauthor:\n  - name: Elham Ali\n    corresponding: true\n    roles:\n      - Researcher\n      - Data Storytelling\n      - Human-centered Design\n      - Data Visualization\n    affiliations:\n      - Beeck Center for Social Impact and Innovation at Georgetown University\nlicense: CC BY-SA 4.0\nkeywords:\n  - civic tech\n  - digital transformation\n  - state government\n  - map\n  - research\ndate: last-modified\nabstract: |\n  This analysis aims to explore trends of digital transformation, specifically on policy, chief data officers, digital service teams, impact reports, and design systems across U.S. states and territories.\nkeypoints:\n  - civic tech\n  - digital transformation\n  - state government\n  - map\n  - research\ncitation:\n  container-title: Beeck Center for Social Impact and Innovation\ndraft: false\nbibliography: references.bib\ncode-fold: true\n# reference-location: margin\n# citation-location: margin\necho: true\nwarning: false\n---\n\n## Background\n\nWhen public climate & EJ evidence disappears (removed, restricted, or altered), a decade of downstream knowledge becomes harder to verify, reproduce, teach, or apply—especially for communities and decisions that most need it.\\\nThis analysis looks at how many studies have used these tools, their topics, and their use cases.\n\n## Questions\n\nHere are the key questions explored in this analysis:\n\n-   How many **states/territories** have at least one:\n\n    -   Executive order\n    -   AI Legislation\n    -   Chief Data Officer (CDO)\n    -   Digital Service Team (DST)\n    -   Digital Service Impact Report\n    -   Design system\n\n-   What is the **distribution of digital transformation activities** by state/territory? (e.g., number of EOs per state)\n\n-   Which states are the **most active vs. least active** across all categories?\n\n-   Which states introduced CDOs or DSTs earliest, and how has that spread across regions?\n\n-   Which states with **active CDOs** also have **design systems** or **DSTs**?\n\n-   Are states with AI legislation more likely to have other forms of digital infrastructure (DST, design systems)?\n\n-   What are the **most common policy topics** in executive orders (e.g., broadband, interoperability, AI)?\n\n-   What are the **most common policy topics** in AI legislations orders (e.g., broadband, interoperability, AI)?\n\n-   What is the **trend over time** in the number of executive orders and AI legislations enacted?\n\n-   Do **early adopters** (states with policies from 2010–2015) differ from late adopters in 2020–2025?\n\n-   Do states with **higher numbers of executive orders** also pass more legislations or rules?\n\n-   Do states with **impact reports** show evidence of greater adoption of other digital practices?\n\n-   Other ideas: Heatmaps of activity types by state, regional comparisons (Northeast vs. South vs. West), and timelines of adoption by year (EOs, bills, rules).\n\n## Data Sources\n\nThe climate tools assessed are:\n\n-   map_data\n-   policy_data\n\nThe data for this project comes from the Digital Service Network at the Beeck and last refreshed on September 30, 2025.\n\nOriginal raw datasets are saved in the `data/` folder. This script reduces and cleans those datasets to prepare them for analysis.\n\n------------------------------------------------------------------------\n\n## Cleaning\n\nI start by loading the packages needed for file handling, data wrangling, and visualization.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n## Folder structure helpers\nlibrary(here)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nhere() starts at /Users/elhamali/Documents/Data Projects/state-of-states-map-analysis\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(ezknitr)\n\n## Data import & cleaning\nlibrary(tidyverse)  \n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(janitor)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(lubridate)\nlibrary(janitor)\n# library(rlang)\n\n## Visualization\nlibrary(highcharter)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(igraph)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'igraph'\n\nThe following objects are masked from 'package:lubridate':\n\n    %--%, union\n\nThe following objects are masked from 'package:dplyr':\n\n    as_data_frame, groups, union\n\nThe following objects are masked from 'package:purrr':\n\n    compose, simplify\n\nThe following object is masked from 'package:tidyr':\n\n    crossing\n\nThe following object is masked from 'package:tibble':\n\n    as_data_frame\n\nThe following objects are masked from 'package:stats':\n\n    decompose, spectrum\n\nThe following object is masked from 'package:base':\n\n    union\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\nlibrary(RColorBrewer)\nlibrary(htmlwidgets)\nlibrary(gt)\n```\n:::\n\n\n### Import raw data\n\nI import all .csv files from the `data/` folder, then save them as .rds files into `output/`. This preserves their structure and speeds up future reads.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\n# List all CSV files\ncsv_files <- list.files(here(\"data\"), pattern = \"\\\\.csv$\", full.names = TRUE)\n\n# Read into a list of dataframes\ndatasets <- map(csv_files, read.csv)\nnames(datasets) <- tools::file_path_sans_ext(basename(csv_files))\n\n# Save each dataset as .rds in output/\nwalk2(\n  datasets,\n  names(datasets),\n  ~ saveRDS(.x, here(\"output\", paste0(.y, \".rds\")))\n)\n```\n:::\n\n\n### Clean both datasets\n\nI apply the same cleaning process to both datasets (map_data and policy_data):\n\n-   Standardize variable names to snake_case\n-   Guess variable types (integers, doubles, dates, etc.)\n-   Convert responses for 'executive_orders', 'ai_legislations', 'digital_service_teams', 'digital_service_impact_reports'\n    -   yes to 1\n    -   no to 0\n-   Convert responses for 'design_systems'\n    -   yes to 1\n    -   in development to 1\n    -   no to 0\n    -   unverified to 0\n-   Convert responses for 'chief_data_officers'\n    -   yes, state CDO to 1\n    -   yes, state CDO equivalent to 1\n    -   no to 0\n    -   no, vacant to 0\n    -   unverified to 0\n-   Add a region variable to classify the state's based on CDC's four regions [^1]\n    -   **Northeast**: Includes Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont\n    -   **Midwest**: Includes Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin\n    -   **South**: Includes Alabama, Arkansas, Delaware, District of Columbia, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, and West Virginia\n    -   **West**: Includes Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.\n    -   **Territories**: Includes American Samoa, Guam, the Northern Mariana Islands, Puerto Rico, and the U.S. Virgin Islands (not included in CDC's regional mapping, but I added here to include the territories)\n\n[^1]: <https://www.cdc.gov/nchs/hus/sources-definitions/geographic-region.htm>\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# -------------------------------------------------------------------\n# Libraries\nlibrary(here)\nlibrary(tidyverse)   # includes dplyr, stringr, readr, etc.\nlibrary(janitor)\nlibrary(lubridate)\nlibrary(forcats)\nlibrary(glue)\nlibrary(fs)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'fs'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following object is masked from 'package:igraph':\n\n    path\n```\n\n\n:::\n\n```{.r .cell-code}\n# -------------------------------------------------------------------\n# Helper: read -> clean names -> guess types -> trim\nread_clean_tag <- function(path) {\n  readRDS(path) |>\n    janitor::clean_names() |>\n    readr::type_convert(col_types = readr::cols(.default = readr::col_guess())) |>\n    dplyr::mutate(across(where(is.character), ~ trimws(.x)))\n}\n\n# Core base cleaner (shared)\nclean_base <- function(df) {\n  df |>\n    janitor::clean_names() |>\n    readr::type_convert(col_types = readr::cols(.default = readr::col_guess())) |>\n    dplyr::mutate(across(where(is.character), ~ trimws(.x)))\n}\n\n# -------------------------------------------------------------------\n# Region mapping (CDC 4 regions + Territories) with robust normalization\nadd_region <- function(df, state_col = \"state_territory\") {\n  norm <- function(x) {\n    x |>\n      stringr::str_to_lower() |>\n      stringr::str_replace_all(\"[[:punct:]]\", \" \") |>\n      stringr::str_squish()\n  }\n\n  northeast <- norm(c(\n    \"Connecticut\",\"Maine\",\"Massachusetts\",\"New Hampshire\",\"New Jersey\",\n    \"New York\",\"Pennsylvania\",\"Rhode Island\",\"Vermont\"\n  ))\n  midwest <- norm(c(\n    \"Illinois\",\"Indiana\",\"Iowa\",\"Kansas\",\"Michigan\",\"Minnesota\",\"Missouri\",\n    \"Nebraska\",\"North Dakota\",\"Ohio\",\"South Dakota\",\"Wisconsin\"\n  ))\n  south <- norm(c(\n    \"Alabama\",\"Arkansas\",\"Delaware\",\"District of Columbia\",\"Florida\",\"Georgia\",\n    \"Kentucky\",\"Louisiana\",\"Maryland\",\"Mississippi\",\"North Carolina\",\"Oklahoma\",\n    \"South Carolina\",\"Tennessee\",\"Texas\",\"Virginia\",\"West Virginia\"\n  ))\n  # DC aliases\n  south <- unique(c(south, norm(c(\"Washington, D.C.\",\"Washington D.C.\",\"Washington DC\",\"DC\"))))\n\n  west <- norm(c(\n    \"Alaska\",\"Arizona\",\"California\",\"Colorado\",\"Hawaii\",\"Idaho\",\"Montana\",\"Nevada\",\n    \"New Mexico\",\"Oregon\",\"Utah\",\"Washington\",\"Wyoming\"\n  ))\n  territories <- norm(c(\n    \"American Samoa\",\"Guam\",\"Northern Mariana Islands\",\"Puerto Rico\",\n    \"U.S. Virgin Islands\",\"US Virgin Islands\",\"Virgin Islands\"\n  ))\n\n  # Choose the state column (fallback to 'state' if needed)\n  chosen_col <-\n    if (state_col %in% names(df)) state_col\n    else if (\"state\" %in% names(df)) \"state\"\n    else if (\"state_territory\" %in% names(df)) \"state_territory\"\n    else NA_character_\n\n  if (is.na(chosen_col)) return(df)\n\n  df |>\n    dplyr::mutate(\n      .key = norm(.data[[chosen_col]]),\n      region = dplyr::case_when(\n        .key %in% northeast   ~ \"Northeast\",\n        .key %in% midwest     ~ \"Midwest\",\n        .key %in% south       ~ \"South\",\n        .key %in% west        ~ \"West\",\n        .key %in% territories ~ \"Territories\",\n        TRUE ~ NA_character_\n      )\n    ) |>\n    dplyr::select(-.key)\n}\n\n# -------------------------------------------------------------------\n# Exact recoding rules for specified columns\n\n# yes/no -> 1/0\nrecode_yes_no <- function(x) {\n  xx <- stringr::str_to_lower(stringr::str_squish(as.character(x)))\n  dplyr::case_when(\n    xx == \"yes\" ~ 1L,\n    xx == \"no\"  ~ 0L,\n    TRUE        ~ NA_integer_\n  )\n}\n\n# design_systems: yes/in development -> 1; no/unverified -> 0\nrecode_design_systems <- function(x) {\n  xx <- stringr::str_to_lower(stringr::str_squish(as.character(x)))\n  dplyr::case_when(\n    xx %in% c(\"yes\",\"in development\",\"in-development\",\"in_development\") ~ 1L,\n    xx %in% c(\"no\",\"unverified\") ~ 0L,\n    TRUE ~ NA_integer_\n  )\n}\n\n# chief_data_officers (CDO):\n# yes, state CDO / yes, state CDO equivalent -> 1\n# no / no, vacant / unverified -> 0\nrecode_cdo <- function(x) {\n  xx <- stringr::str_to_lower(stringr::str_squish(as.character(x)))\n  dplyr::case_when(\n    xx %in% c(\"yes, state cdo\",\"yes, state cdo equivalent\") ~ 1L,\n    xx %in% c(\"no\",\"no, vacant\",\"unverified\") ~ 0L,\n    TRUE ~ NA_integer_\n  )\n}\n\n# Apply recoding only to the specified columns if they exist\nrecode_specified_status_cols <- function(df) {\n  df |>\n    dplyr::mutate(\n      dplyr::across(\n        intersect(c(\"executive_orders\", \"ai_legislations\",\n                    \"digital_service_teams\", \"digital_service_impact_reports\"),\n                  names(df)),\n        recode_yes_no\n      ),\n      dplyr::across(\n        intersect(\"design_systems\", names(df)),\n        recode_design_systems\n      ),\n      dplyr::across(\n        intersect(\"chief_data_officers\", names(df)),\n        recode_cdo\n      )\n    )\n}\n\n# -------------------------------------------------------------------\n# One canonical cleaner to use everywhere\nclean_dataset <- function(df) {\n  df |>\n    clean_base() |>\n    recode_specified_status_cols() |>\n    add_region()\n}\n\n# -------------------------------------------------------------------\n# Read raw RDS (from your earlier import step), clean, write outputs\nmap_raw    <- read_clean_tag(here(\"output\", \"map_data.rds\"))\npolicy_raw <- read_clean_tag(here(\"output\", \"policy_data.rds\"))\n\nmap_clean    <- clean_dataset(map_raw)\npolicy_clean <- clean_dataset(policy_raw)\n\nfs::dir_create(here(\"output\"))\nreadr::write_rds(map_clean,    here(\"output\", \"map_data_clean.rds\"))\nreadr::write_rds(policy_clean, here(\"output\", \"policy_data_clean.rds\"))\nreadr::write_csv(map_clean,    here(\"output\", \"map_data_clean.csv\"), na = \"\")\nreadr::write_csv(policy_clean, here(\"output\", \"policy_data_clean.csv\"), na = \"\")\n```\n:::\n\n\nI'll now assign the right variable types for the cleaned datasets.\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(readr)\nlibrary(forcats)\n\n# load inputs created earlier\nmap_data_clean    <- readr::read_rds(here(\"output\", \"map_data_clean.rds\"))\npolicy_data_clean <- readr::read_rds(here(\"output\", \"policy_data_clean.rds\"))\n\nto_binary <- function(x) {\n  if (is.logical(x)) return(as.integer(x))\n  if (is.numeric(x)) return(as.integer(x == 1))\n  x_chr <- tolower(trimws(as.character(x)))\n  x_chr[x_chr == \"\"] <- NA\n  yes_vals <- c(\"yes\",\"y\",\"true\",\"t\",\"1\",\"present\",\"has\",\"active\",\"in development\")\n  no_vals  <- c(\"no\",\"n\",\"false\",\"f\",\"0\",\"absent\",\"none\",\"inactive\",\"unverified\",\"no, vacant\")\n  as.integer(dplyr::case_when(\n    x_chr %in% yes_vals ~ 1L,\n    x_chr %in% no_vals  ~ 0L,\n    TRUE ~ NA_integer_\n  ))\n}\n\nto_region_factor <- function(x) {\n  # map region labels to codes 1..5\n  ref <- c(\"Northeast\",\"Midwest\",\"South\",\"West\",\"Territories\")\n  idx <- match(as.character(x), ref)          # NA if label not in ref\n  factor(as.character(idx), levels = as.character(seq_along(ref)), ordered = TRUE)\n}\n\n# ---------- map_data_clean ----------\nmap_int_cols <- c(\n  \"total_number_of_executive_orders\",\n  \"total_number_of_legislation\",\n  \"total_number_of_administrative_rules\",\n  \"publication_year\"\n)\nmap_dbl_cols <- c(\"cdo_year_established\")\nmap_bin_cols <- c(\n  \"executive_orders\",\"ai_legislations\",\"administrative_rules\",\n  \"digital_service_teams\",\"digital_service_impact_reports\",\n  \"chief_data_officers\",\"design_systems\",\"design_system_open_source_status\"\n)\nmap_cat_cols <- c(\"bill_status\")\n\nmap_data_clean <- map_data_clean %>%\n  mutate(across(everything(), ~ as.character(.))) %>%\n  mutate(\n    across(any_of(map_int_cols), ~ suppressWarnings(as.integer(readr::parse_number(.)))),\n    across(any_of(map_dbl_cols), ~ suppressWarnings(as.double(readr::parse_number(.)))),\n    across(any_of(map_bin_cols), to_binary),\n    across(any_of(map_cat_cols), ~ forcats::as_factor(trimws(.))),\n    region = to_region_factor(region)\n  )\n\n# ---------- policy_data_clean ----------\npol_int_cols <- c(\"executive_order_year_enacted\", \"legislative_session\")\npol_cat_cols <- c(\"scan_type\", \"bill_status\")\n\npolicy_data_clean <- policy_data_clean %>%\n  mutate(across(everything(), ~ as.character(.))) %>%\n  mutate(\n    across(any_of(pol_int_cols), ~ suppressWarnings(as.integer(readr::parse_number(.)))),\n    across(any_of(pol_cat_cols), ~ forcats::as_factor(trimws(.))),\n    region = to_region_factor(region)\n  )\n```\n:::\n\n\n## Analysis\n\nI will look at each question one by one and clean the data as I go. I will organize the data during the analysis before exploring the results. I'll also export intermediate results into tidy CSV files so they are ready for further visualization and exploration.\n\n-   How many **states/territories** have at least one:\n\n    -   Executive order\n    -   AI Legislation\n    -   Chief Data Officer (CDO)\n    -   Digital Service Team (DST)\n    -   Digital Service Impact Report\n    -   Design system\n\n\n::: {.cell}\n\n```{.r .cell-code .hidden}\nlibrary(readr); library(here)\nmap_data_clean <- read_rds(here(\"output\", \"map_data_clean.rds\"))\n\n\n# Packages\nlibrary(tidyverse)\nlibrary(highcharter)\nlibrary(htmlwidgets)\nlibrary(webshot2)\nlibrary(glue)\nlibrary(scales)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\n\nAttaching package: 'scales'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following object is masked from 'package:purrr':\n\n    discard\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nThe following object is masked from 'package:readr':\n\n    col_factor\n```\n\n\n:::\n\n```{.r .cell-code .hidden}\n# Indicators to analyze\nindicators <- c(\n  executive_orders                = \"Executive order\",\n  ai_legislations                = \"AI legislation\",\n  chief_data_officers            = \"Chief Data Officer (CDO)\",\n  digital_service_teams          = \"Digital Service Team (DST)\",\n  digital_service_impact_reports = \"Digital Service Impact Report\",\n  design_systems                 = \"Design system\"\n)\n\n# Prep: unique states; turn binary fields into logical presence\n# Works whether columns are 0/1 integers or TRUE/FALSE logicals\nmap_prepped <- map_data_clean %>%\n  distinct(state_territory, .keep_all = TRUE) %>%\n  mutate(\n    across(\n      any_of(names(indicators)),\n      ~ dplyr::coalesce(. == 1L, FALSE)  # TRUE if value == 1; NA -> FALSE\n    )\n  )\n\nn_states <- nrow(map_prepped)\n\n# Summaries\ncounts_tbl <- map_prepped %>%\n  select(any_of(names(indicators))) %>%\n  pivot_longer(everything(), names_to = \"indicator\", values_to = \"has_it\") %>%\n  group_by(indicator) %>%\n  summarise(\n    count   = sum(has_it, na.rm = TRUE),\n    percent = 100 * count / n_states,\n    .groups = \"drop\"\n  ) %>%\n  mutate(label = recode(indicator, !!!indicators))\n\n# Ordered statements\norder_vec <- c(\n  \"executive_orders\",\n  \"ai_legislations\",\n  \"chief_data_officers\",\n  \"design_systems\",\n  \"digital_service_teams\",\n  \"digital_service_impact_reports\"\n)\n\ninsights <- counts_tbl %>%\n  filter(indicator %in% order_vec) %>%\n  mutate(\n    statement = case_when(\n      indicator == \"executive_orders\" ~ glue(\"{number(percent, accuracy = 0.1)}% U.S. states/territories have an executive order.\"),\n      indicator == \"ai_legislations\" ~ glue(\"{number(percent, accuracy = 0.1)}% U.S. states/territories have AI legislation.\"),\n      indicator == \"chief_data_officers\" ~ glue(\"{number(percent, accuracy = 0.1)}% U.S. states/territories have a chief data office (CDO).\"),\n      indicator == \"design_systems\" ~ glue(\"{number(percent, accuracy = 0.1)}% U.S. states/territories have a design system.\"),\n      indicator == \"digital_service_teams\" ~ glue(\"{number(percent, accuracy = 0.1)}% U.S. states/territories have a digital service team (DST).\"),\n      indicator == \"digital_service_impact_reports\" ~ glue(\"{number(percent, accuracy = 0.1)}% U.S. states/territories have a digital service impact report.\")\n    )\n  ) %>%\n  select(indicator, label, count, percent, statement) %>%\n  mutate(percent = round(percent, 1)) %>%\n  arrange(match(indicator, order_vec))\n\n# Save insights\nreadr::write_csv(insights, here(\"output\", \"insights.csv\"))\n\n# Chart\ncounts_for_chart <- counts_tbl %>% arrange(desc(count))\n\nhc <- highchart() %>%\n  hc_chart(type = \"column\") %>%\n  hc_title(text = \"States/Territories with at least one item\") %>%\n  hc_subtitle(text = paste0(\"n = \", n_states, \" states/territories\")) %>%\n  hc_xAxis(categories = counts_for_chart$label, title = list(text = NULL)) %>%\n  hc_yAxis(title = list(text = \"Count\"), allowDecimals = FALSE, min = 0,\n           max = max(counts_for_chart$count)) %>%\n  hc_series(\n    list(\n      name = \"Count\",\n      data = counts_for_chart$count,\n      dataLabels = list(enabled = TRUE),\n      tooltip = list(pointFormat = paste0(\"<b>{point.y}</b> of \", n_states))\n    )\n  ) %>%\n  hc_plotOptions(column = list(borderRadius = 3, pointPadding = 0.05, groupPadding = 0.1)) %>%\n  hc_exporting(enabled = TRUE)\n\nsaveWidget(hc, file = here(\"output\", \"q1.html\"), selfcontained = TRUE)\nwebshot2::webshot(here(\"output\", \"q1.html\"), file = here(\"output\", \"q1.png\"),\n                  vwidth = 900, vheight = 600)\n```\n\n::: {.cell-output .cell-output-stderr .hidden}\n\n```\nfile:////Users/elhamali/Documents/Data Projects/state-of-states-map-analysis/output/q1.html screenshot completed\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](analysis_files/figure-html/research-question-1-1.png){width=672}\n:::\n\n```{.r .cell-code .hidden}\ninsights$statement\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n83.9% U.S. states/territories have an executive order.\n55.4% U.S. states/territories have AI legislation.\n51.8% U.S. states/territories have a chief data office (CDO).\n33.9% U.S. states/territories have a design system.\n25.0% U.S. states/territories have a digital service team (DST).\n12.5% U.S. states/territories have a digital service impact report.\n```\n\n\n:::\n:::\n\n\n-   Which **regions** have the most vs least:\n\n    -   Executive order\n\n    -   AI Legislations\n\n    -   Chief Data Officers (CDO)\n\n    -   Digital Service Teams (DST)\n\n    -   Digital Service Impact Reports\n\n    -   Design systems\n\n\n::: {.cell}\n\n:::\n\n\n-   What is the **distribution of digital transformation activities** by state/territory? (e.g., number of EOs per state)\n\n\n::: {.cell}\n\n:::\n\n\n-   Which states are the **most active vs. least active** across all categories?\n\n-   Which states introduced CDOs or DSTs earliest, and how has that spread across regions?\n\n-   Which states with **active CDOs** also have **design systems** or **DSTs**?\n\n-   Are states with AI legislation more likely to have other forms of digital infrastructure (DST, design systems)?\n\n-   What are the **most common policy topics** in executive orders (e.g., broadband, interoperability, AI)?\n\n-   What are the **most common policy topics** in AI legislations orders (e.g., broadband, interoperability, AI)?\n\n-   What is the **trend over time** in the number of executive orders and AI legislations enacted?\n\n-   Do **early adopters** (states with policies from 2010–2015) differ from late adopters in 2020–2025?\n\n-   Do states with **higher numbers of executive orders** also pass more legislations or rules?\n\n-   Do states with **impact reports** show evidence of greater adoption of other digital practices?\n\n-   Other ideas: Heatmaps of activity types by state, regional comparisons (Northeast vs. South vs. West), and timelines of adoption by year (EOs, bills, rules).\n",
    "supporting": [
      "analysis_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
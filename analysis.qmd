---
title: State of the States map analysis
author:
  - name: Elham Ali
    corresponding: true
    roles:
      - Researcher
      - Data Storytelling
      - Human-centered Design
      - Data Visualization
    affiliations:
      - Beeck Center for Social Impact and Innovation at Georgetown University
license: CC BY-SA 4.0
keywords:
  - civic tech
  - digital transformation
  - state government
  - map
  - research
date: last-modified
abstract: |
  This analysis aims to explore trends of digital transformation, specifically on policy, chief data officers, digital service teams, impact reports, and design systems across U.S. states and territories.
keypoints:
  - civic tech
  - digital transformation
  - state government
  - map
  - research
citation:
  container-title: Beeck Center for Social Impact and Innovation
draft: false
bibliography: references.bib
code-fold: true
# reference-location: margin
# citation-location: margin
echo: true
warning: false
---

## Background

When public climate & EJ evidence disappears (removed, restricted, or altered), a decade of downstream knowledge becomes harder to verify, reproduce, teach, or apply—especially for communities and decisions that most need it.\
This analysis looks at how many studies have used these tools, their topics, and their use cases.

## Questions

Here are the key questions explored in this analysis:

-   How many **states/territories** have at least one:

    -   Executive order
    -   AI Legislation
    -   Chief Data Officer (CDO)
    -   Digital Service Team (DST)
    -   Digital Service Impact Report
    -   Design system

-   Which states are the **most active vs. least active** across all categories?

-   Which states introduced **CDOs or DSTs earliest**, and how has that spread across regions?

-   Which states with **active CDOs** also have **design systems** or **DSTs**?

-   Are states with **AI legislation** more likely to have other forms of digital infrastructure (DST, design systems)?

-   In which region(s) do **AI legislation policies cluster** in?

-   What are the **most common policy topics** in executive orders (e.g., broadband, interoperability, AI)?

-   What are the **most common policy topics** in AI legislations orders (e.g., broadband, interoperability, AI)?

-   What is the **trend over time** in the number of executive orders and AI legislations enacted?

-   Do **early adopters** (states with policies from 2010–2015) differ from **late adopters** in 2020–2025?

-   Do certain AI legislation topics **cluster in certain regions**? Do certain executive topics topics cluster in certain regions?

## Data Sources

The climate tools assessed are:

-   map_data
-   policy_data

The data for this project comes from the Digital Service Network at the Beeck and last refreshed on September 30, 2025.

Original raw datasets are saved in the `data/` folder. This script reduces and cleans those datasets to prepare them for analysis.

------------------------------------------------------------------------

## Cleaning

I start by loading the packages needed for file handling, data wrangling, and visualization.

```{r}
#| label: load-libraries

## Folder structure helpers
library(here)
library(ezknitr)

## Data import & cleaning
library(tidyverse)  
library(janitor)
library(lubridate)
library(janitor)
# library(rlang)

## Visualization
library(highcharter)
library(igraph)
library(RColorBrewer)
library(htmlwidgets)
library(gt)
```

### Import raw data

I import all .csv files from the `data/` folder, then save them as .rds files into `output/`. This preserves their structure and speeds up future reads.

```{r}
#| label: import-data

# List all CSV files
csv_files <- list.files(here("data"), pattern = "\\.csv$", full.names = TRUE)

# Read into a list of dataframes
datasets <- map(csv_files, read.csv)
names(datasets) <- tools::file_path_sans_ext(basename(csv_files))

# Save each dataset as .rds in output/
walk2(
  datasets,
  names(datasets),
  ~ saveRDS(.x, here("output", paste0(.y, ".rds")))
)
```

### Clean both datasets

I apply the same cleaning process to both datasets (map_data and policy_data):

-   Standardize variable names to snake_case
-   Guess variable types (integers, doubles, dates, etc.)
-   Convert responses for 'executive_orders', 'ai_legislations', 'digital_service_teams', 'digital_service_impact_reports'
    -   yes to 1
    -   no to 0
-   Convert responses for 'design_systems'
    -   yes to 1
    -   in development to 1
    -   no to 0
    -   unverified to 0
-   Convert responses for 'chief_data_officers'
    -   yes, state CDO to 1
    -   yes, state CDO equivalent to 1
    -   no to 0
    -   no, vacant to 0
    -   unverified to 0
-   Add a region variable to classify the state's based on CDC's four regions [^1]
    -   **Northeast**: Includes Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont
    -   **Midwest**: Includes Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin
    -   **South**: Includes Alabama, Arkansas, Delaware, District of Columbia, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, and West Virginia
    -   **West**: Includes Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.
    -   **Territories**: Includes American Samoa, Guam, the Northern Mariana Islands, Puerto Rico, and the U.S. Virgin Islands (not included in CDC's regional mapping, but I added here to include the territories)

[^1]: <https://www.cdc.gov/nchs/hus/sources-definitions/geographic-region.htm>

```{r echo=TRUE, message=FALSE, results='markup'}
#| label: clean-datasets

# -------------------------------------------------------------------
# Libraries
library(here)
library(tidyverse)   # includes dplyr, stringr, readr, etc.
library(janitor)
library(lubridate)
library(forcats)
library(glue)
library(fs)

# -------------------------------------------------------------------
# Helper: read -> clean names -> guess types -> trim
read_clean_tag <- function(path) {
  readRDS(path) |>
    janitor::clean_names() |>
    readr::type_convert(col_types = readr::cols(.default = readr::col_guess())) |>
    dplyr::mutate(across(where(is.character), ~ trimws(.x)))
}

# Core base cleaner (shared)
clean_base <- function(df) {
  df |>
    janitor::clean_names() |>
    readr::type_convert(col_types = readr::cols(.default = readr::col_guess())) |>
    dplyr::mutate(across(where(is.character), ~ trimws(.x)))
}

# -------------------------------------------------------------------
# Region mapping (CDC 4 regions + Territories) with robust normalization
add_region <- function(df, state_col = "state_territory") {
  norm <- function(x) {
    x |>
      stringr::str_to_lower() |>
      stringr::str_replace_all("[[:punct:]]", " ") |>
      stringr::str_squish()
  }

  northeast <- norm(c(
    "Connecticut","Maine","Massachusetts","New Hampshire","New Jersey",
    "New York","Pennsylvania","Rhode Island","Vermont"
  ))
  midwest <- norm(c(
    "Illinois","Indiana","Iowa","Kansas","Michigan","Minnesota","Missouri",
    "Nebraska","North Dakota","Ohio","South Dakota","Wisconsin"
  ))
  south <- norm(c(
    "Alabama","Arkansas","Delaware","District of Columbia","Florida","Georgia",
    "Kentucky","Louisiana","Maryland","Mississippi","North Carolina","Oklahoma",
    "South Carolina","Tennessee","Texas","Virginia","West Virginia"
  ))
  # DC aliases
  south <- unique(c(south, norm(c("Washington, D.C.","Washington D.C.","Washington DC","DC"))))

  west <- norm(c(
    "Alaska","Arizona","California","Colorado","Hawaii","Idaho","Montana","Nevada",
    "New Mexico","Oregon","Utah","Washington","Wyoming"
  ))
  territories <- norm(c(
    "American Samoa","Guam","Northern Mariana Islands","Puerto Rico",
    "U.S. Virgin Islands","US Virgin Islands","Virgin Islands"
  ))

  # Choose the state column (fallback to 'state' if needed)
  chosen_col <-
    if (state_col %in% names(df)) state_col
    else if ("state" %in% names(df)) "state"
    else if ("state_territory" %in% names(df)) "state_territory"
    else NA_character_

  if (is.na(chosen_col)) return(df)

  df |>
    dplyr::mutate(
      .key = norm(.data[[chosen_col]]),
      region = dplyr::case_when(
        .key %in% northeast   ~ "Northeast",
        .key %in% midwest     ~ "Midwest",
        .key %in% south       ~ "South",
        .key %in% west        ~ "West",
        .key %in% territories ~ "Territories",
        TRUE ~ NA_character_
      )
    ) |>
    dplyr::select(-.key)
}

# -------------------------------------------------------------------
# Exact recoding rules for specified columns

# yes/no -> 1/0
recode_yes_no <- function(x) {
  xx <- stringr::str_to_lower(stringr::str_squish(as.character(x)))
  dplyr::case_when(
    xx == "yes" ~ 1L,
    xx == "no"  ~ 0L,
    TRUE        ~ NA_integer_
  )
}

# design_systems: yes/in development -> 1; no/unverified -> 0
recode_design_systems <- function(x) {
  xx <- stringr::str_to_lower(stringr::str_squish(as.character(x)))
  dplyr::case_when(
    xx %in% c("yes","in development","in-development","in_development") ~ 1L,
    xx %in% c("no","unverified") ~ 0L,
    TRUE ~ NA_integer_
  )
}

# chief_data_officers (CDO):
# yes, state CDO / yes, state CDO equivalent -> 1
# no / no, vacant / unverified -> 0
recode_cdo <- function(x) {
  xx <- stringr::str_to_lower(stringr::str_squish(as.character(x)))
  dplyr::case_when(
    xx %in% c("yes, state cdo","yes, state cdo equivalent") ~ 1L,
    xx %in% c("no","no, vacant","unverified") ~ 0L,
    TRUE ~ NA_integer_
  )
}

# Apply recoding only to the specified columns if they exist
recode_specified_status_cols <- function(df) {
  df |>
    dplyr::mutate(
      dplyr::across(
        intersect(c("executive_orders", "ai_legislations",
                    "digital_service_teams", "digital_service_impact_reports"),
                  names(df)),
        recode_yes_no
      ),
      dplyr::across(
        intersect("design_systems", names(df)),
        recode_design_systems
      ),
      dplyr::across(
        intersect("chief_data_officers", names(df)),
        recode_cdo
      )
    )
}

# -------------------------------------------------------------------
# One canonical cleaner to use everywhere
clean_dataset <- function(df) {
  df |>
    clean_base() |>
    recode_specified_status_cols() |>
    add_region()
}

# -------------------------------------------------------------------
# Read raw RDS (from your earlier import step), clean, write outputs
map_raw    <- read_clean_tag(here("output", "map_data.rds"))
policy_raw <- read_clean_tag(here("output", "policy_data.rds"))

map_clean    <- clean_dataset(map_raw)
policy_clean <- clean_dataset(policy_raw)

fs::dir_create(here("output"))
readr::write_rds(map_clean,    here("output", "map_data_clean.rds"))
readr::write_rds(policy_clean, here("output", "policy_data_clean.rds"))
readr::write_csv(map_clean,    here("output", "map_data_clean.csv"), na = "")
readr::write_csv(policy_clean, here("output", "policy_data_clean.csv"), na = "")
```

I'll now assign the right variable types for the cleaned datasets.

```{r}
#| label: assign-variables

library(dplyr)
library(stringr)
library(readr)
library(forcats)

# load inputs created earlier
map_data_clean    <- readr::read_rds(here("output", "map_data_clean.rds"))
policy_data_clean <- readr::read_rds(here("output", "policy_data_clean.rds"))

to_binary <- function(x) {
  if (is.logical(x)) return(as.integer(x))
  if (is.numeric(x)) return(as.integer(x == 1))
  x_chr <- tolower(trimws(as.character(x)))
  x_chr[x_chr == ""] <- NA
  yes_vals <- c("yes","y","true","t","1","present","has","active","in development")
  no_vals  <- c("no","n","false","f","0","absent","none","inactive","unverified","no, vacant")
  as.integer(dplyr::case_when(
    x_chr %in% yes_vals ~ 1L,
    x_chr %in% no_vals  ~ 0L,
    TRUE ~ NA_integer_
  ))
}

to_region_factor <- function(x) {
  # map region labels to codes 1..5
  ref <- c("Northeast","Midwest","South","West","Territories")
  idx <- match(as.character(x), ref)          # NA if label not in ref
  factor(as.character(idx), levels = as.character(seq_along(ref)), ordered = TRUE)
}

# ---------- map_data_clean ----------
map_int_cols <- c(
  "total_number_of_executive_orders",
  "total_number_of_legislation",
  "total_number_of_administrative_rules",
  "publication_year"
)
map_dbl_cols <- c("cdo_year_established")
map_bin_cols <- c(
  "executive_orders","ai_legislations","administrative_rules",
  "digital_service_teams","digital_service_impact_reports",
  "chief_data_officers","design_systems","design_system_open_source_status"
)
map_cat_cols <- c("bill_status")

map_data_clean <- map_data_clean %>%
  mutate(across(everything(), ~ as.character(.))) %>%
  mutate(
    across(any_of(map_int_cols), ~ suppressWarnings(as.integer(readr::parse_number(.)))),
    across(any_of(map_dbl_cols), ~ suppressWarnings(as.double(readr::parse_number(.)))),
    across(any_of(map_bin_cols), to_binary),
    across(any_of(map_cat_cols), ~ forcats::as_factor(trimws(.))),
    region = to_region_factor(region)
  )

# ---------- policy_data_clean ----------
pol_int_cols <- c("executive_order_year_enacted", "legislative_session")
pol_cat_cols <- c("scan_type", "bill_status")

policy_data_clean <- policy_data_clean %>%
  mutate(across(everything(), ~ as.character(.))) %>%
  mutate(
    across(any_of(pol_int_cols), ~ suppressWarnings(as.integer(readr::parse_number(.)))),
    across(any_of(pol_cat_cols), ~ forcats::as_factor(trimws(.))),
    region = to_region_factor(region)
  )
```

## Analysis

I will look at each question one by one and clean the data as I go. I will organize the data during the analysis before exploring the results. I'll also export intermediate results into tidy CSV files so they are ready for further visualization and exploration.

### Q1

-   How many **states/territories** have at least one:

    -   Executive order
    -   AI Legislation
    -   Chief Data Officer (CDO)
    -   Digital Service Team (DST)
    -   Digital Service Impact Report
    -   Design system

```{r}
#| label: research-question-1
#| message: false
#| warning: false

map_data_clean <- read_rds(here("output", "map_data_clean.rds"))

# Packages
library(tidyverse)
library(highcharter)
library(htmlwidgets)
library(webshot2)
library(glue)
library(scales)

# Indicators to analyze
indicators <- c(
  executive_orders                = "Executive order",
  ai_legislations                = "AI legislation",
  chief_data_officers            = "Chief Data Officer (CDO)",
  digital_service_teams          = "Digital Service Team (DST)",
  digital_service_impact_reports = "Digital Service Impact Report",
  design_systems                 = "Design system"
)

# Prep: unique states; turn binary fields into logical presence
# Works whether columns are 0/1 integers or TRUE/FALSE logicals
map_prepped <- map_data_clean %>%
  distinct(state_territory, .keep_all = TRUE) %>%
  mutate(
    across(
      any_of(names(indicators)),
      ~ dplyr::coalesce(. == 1L, FALSE)  # TRUE if value == 1; NA -> FALSE
    )
  )

n_states <- nrow(map_prepped)

# Summaries
counts_tbl <- map_prepped %>%
  select(any_of(names(indicators))) %>%
  pivot_longer(everything(), names_to = "indicator", values_to = "has_it") %>%
  group_by(indicator) %>%
  summarise(
    count   = sum(has_it, na.rm = TRUE),
    percent = 100 * count / n_states,
    .groups = "drop"
  ) %>%
  mutate(label = recode(indicator, !!!indicators))

# Ordered statements
order_vec <- c(
  "executive_orders",
  "ai_legislations",
  "chief_data_officers",
  "design_systems",
  "digital_service_teams",
  "digital_service_impact_reports"
)

insights <- counts_tbl %>%
  filter(indicator %in% order_vec) %>%
  mutate(
    statement = case_when(
      indicator == "executive_orders" ~ glue("{number(percent, accuracy = 0.1)}% U.S. states/territories have an executive order."),
      indicator == "ai_legislations" ~ glue("{number(percent, accuracy = 0.1)}% U.S. states/territories have AI legislation."),
      indicator == "chief_data_officers" ~ glue("{number(percent, accuracy = 0.1)}% U.S. states/territories have a chief data office (CDO)."),
      indicator == "design_systems" ~ glue("{number(percent, accuracy = 0.1)}% U.S. states/territories have a design system."),
      indicator == "digital_service_teams" ~ glue("{number(percent, accuracy = 0.1)}% U.S. states/territories have a digital service team (DST)."),
      indicator == "digital_service_impact_reports" ~ glue("{number(percent, accuracy = 0.1)}% U.S. states/territories have a digital service impact report.")
    )
  ) %>%
  select(indicator, label, count, percent, statement) %>%
  mutate(percent = round(percent, 1)) %>%
  arrange(match(indicator, order_vec))

# Save insights
readr::write_csv(insights, here("output", "insights.csv"))

# Chart
counts_for_chart <- counts_tbl %>% arrange(desc(count))

hc <- highchart() %>%
  hc_chart(type = "column") %>%
  hc_title(text = "States/Territories with at least one item") %>%
  hc_subtitle(text = paste0("n = ", n_states, " states/territories")) %>%
  hc_xAxis(categories = counts_for_chart$label, title = list(text = NULL)) %>%
  hc_yAxis(title = list(text = "Count"), allowDecimals = FALSE, min = 0,
           max = max(counts_for_chart$count)) %>%
  hc_series(
    list(
      name = "Count",
      data = counts_for_chart$count,
      dataLabels = list(enabled = TRUE),
      tooltip = list(pointFormat = paste0("<b>{point.y}</b> of ", n_states))
    )
  ) %>%
  hc_plotOptions(column = list(borderRadius = 3, pointPadding = 0.05, groupPadding = 0.1)) %>%
  hc_exporting(enabled = TRUE)

saveWidget(hc, file = here("output", "q1.html"), selfcontained = TRUE)
webshot2::webshot(here("output", "q1.html"), file = here("output", "q1.png"),
                  vwidth = 900, vheight = 600)

insights$statement
```

### Q2

-   Which **regions** have the most vs least:

    -   Executive order

    -   AI Legislations

    -   Chief Data Officers (CDO)

    -   Digital Service Teams (DST)

    -   Digital Service Impact Reports

    -   Design systems

I grouped states and territories into CDC regions and flagged whether each had an executive order, AI legislation, a Chief Data Officer (CDO), a Digital Service Team (DST), a Digital Service Impact Report, or a design system. I then summed counts within each region to identify where adoption was highest and lowest.

```{r}
#| label: research-question-2
#| message: false
#| warning: false

library(readr); library(here)
library(dplyr); library(tidyr); library(purrr)
library(glue)

map_data_clean <- read_rds(here("output", "map_data_clean.rds"))

# Indicators
indicators <- c(
  executive_orders                = "Executive order",
  ai_legislations                 = "AI legislation",
  chief_data_officers             = "Chief Data Officer (CDO)",
  digital_service_teams           = "Digital Service Team (DST)",
  digital_service_impact_reports  = "Digital Service Impact Report",
  design_systems                  = "Design system"
)

region_order <- c("Northeast","Midwest","South","West","Territories")

# Prep: one row per state, coerce to logical
map_prepped <- map_data_clean %>%
  distinct(state_territory, .keep_all = TRUE) %>%
  filter(!is.na(region)) %>%
  mutate(region = factor(region, levels = region_order)) %>%
  mutate(across(any_of(names(indicators)),
                ~ dplyr::coalesce(. == 1L | . == TRUE, FALSE)))

# Counts by region & indicator
region_counts <- map_prepped %>%
  select(region, any_of(names(indicators))) %>%
  pivot_longer(-region, names_to = "indicator", values_to = "has_it") %>%
  group_by(region, indicator) %>%
  summarise(count = sum(has_it, na.rm = TRUE), .groups = "drop") %>%
  mutate(label = recode(indicator, !!!indicators)) %>%
  arrange(label, region)

# Find most/least per indicator
most_tbl <- region_counts %>%
  group_by(indicator) %>%
  slice_max(count, with_ties = TRUE) %>%
  ungroup() %>%
  mutate(which = "most")

least_tbl <- region_counts %>%
  group_by(indicator) %>%
  slice_min(count, with_ties = TRUE) %>%
  ungroup() %>%
  mutate(which = "least")

# Build sentences
pretty_regions <- function(x) paste(x, collapse = ", ")
mk_sentence <- function(rows, which, obj) {
  regs <- pretty_regions(rows$region)
  glue("{regs} has the {which} {obj}.")
}

insights_q2 <- tibble(indicator = unique(region_counts$indicator)) %>%
  mutate(
    label = recode(indicator, !!!indicators),
    statement = map_chr(indicator, \(ind) {
      most  <- filter(most_tbl,  indicator == ind)
      least <- filter(least_tbl, indicator == ind)
      paste(
        mk_sentence(most, "most",  recode(ind, !!!indicators)),
        mk_sentence(least, "least", recode(ind, !!!indicators))
      )
    })
  ) %>%
  select(label, statement)

# Save both outputs
write_csv(region_counts, here("output", "region_counts_q2.csv"))
write_csv(insights_q2,  here("output", "insights_q2.csv"))

# Print results in document
# region_counts
insights_q2
```

### Q3

-   Which states are the **most active vs. least active** across all categories?

To measure overall state activity, I created a six-item score where states received one point for the presence of each digital activity (executive orders, AI legislation, CDO, DST, Digital Service Impact Report, design system). Each state’s score ranged from 0–6 to indicate the breadth of digital transformation. I then ranked states by total score and identified the most and least active states.

```{r}
#| label: research-question-3
#| message: false
#| warning: false

library(readr); library(here)
library(dplyr); library(tidyr); library(purrr); library(glue)

# 1) Load
map_data_clean <- read_rds(here("output", "map_data_clean.rds"))

# 2) Columns (names = dataset column names; values = friendly labels)
act_labels <- c(
  executive_orders               = "Executive orders",
  ai_legislations                = "AI legislations",
  chief_data_officers            = "Chief Data Officers (CDO)",
  digital_service_teams          = "Digital Service Teams (DST)",
  digital_service_impact_reports = "Digital Service Impact Reports",
  design_systems                 = "Design systems"
)

# 3) Presence (0/1) per category, score out of 6 (one row per state)
state_counts_q3 <- map_data_clean %>%
  distinct(state_territory, .keep_all = TRUE) %>%
  transmute(
    state_territory,
    across(
      all_of(names(act_labels)),
      ~ as.integer(dplyr::coalesce(. >= 1L, FALSE))
    )
  ) %>%
  mutate(
    activity_score = rowSums(across(all_of(names(act_labels)))),
    categories_present = pmap_chr(
      across(all_of(names(act_labels))),
      ~ {
        vals <- c(...)
        labs <- act_labels[vals == 1]
        if (length(labs) == 0) "None" else paste(labs, collapse = ", ")
      }
    )
  ) %>%
  arrange(desc(activity_score), state_territory)

# 4) Full ranks (for later viz)
state_ranks_q3 <- state_counts_q3 %>%
  mutate(
    rank_desc = min_rank(desc(activity_score)),  # 1 = most active
    rank_asc  = min_rank(activity_score)        # 1 = least active
  ) %>%
  arrange(rank_desc, state_territory)

# 5) Top/bottom 10 (ties included at the cutoff)
top10 <- state_ranks_q3 %>% filter(rank_desc <= 10)
bot10 <- state_ranks_q3 %>% filter(rank_asc  <= 10)

fmt_list <- function(df) paste0(df$state_territory, " (", df$activity_score, "/6)", collapse = "; ")

insights_q3 <- tibble(
  type = c("Most active (top 10; ties included)", "Least active (bottom 10; ties included)"),
  statement = c(fmt_list(top10), fmt_list(bot10))
)

# 6) Save (match Q2 naming pattern: *_q3.csv) -------------------------------
if (!dir.exists(here("output"))) dir.create(here("output"), recursive = TRUE)
write_csv(state_counts_q3, here("output", "state_counts_q3.csv"))
write_csv(insights_q3,     here("output", "insights_q3.csv"))
write_csv(state_ranks_q3,  here("output", "state_ranks_q3.csv"))

# 7) Print in document (same pattern as Q2)
# state_counts_q3
insights_q3
```

### Q4

-   Which states introduced **CDOs or DSTs earliest**, and how has that spread across regions?

For CDOs, I filtered to states with an active CDO and recorded the year the office was established. I then identified the earliest and most recent adopters. For DSTs, I imported a separate dataset of team founding years and names, cleaned it, and identified the earliest and latest established teams.

```{r}
#| label: research-question-4
#| message: false
#| warning: false

library(readr); library(here)
library(dplyr); library(glue)

# -------------------------------------------------------------------
# 1) Load map_data_clean
map_data_clean <- read_rds(here("output", "map_data_clean.rds"))

# -------------------------------------------------------------------
# 2) Import and clean dst_data.csv -> dst_data_clean
dst_data_clean <- read_csv(here("data", "dst_data.csv"),
                           show_col_types = FALSE) %>%
  mutate(
    team_year_founded = as.integer(team_year_founded),
    state_territory   = as.character(state_territory),
    team_name         = as.character(team_name)
  )

# Save cleaned DST data
write_rds(dst_data_clean, here("output", "dst_data_clean.rds"))
write_csv(dst_data_clean, here("output", "dst_data_clean.csv"))

# -------------------------------------------------------------------
# 3) CDO analysis
cdo_tbl <- map_data_clean %>%
  distinct(state_territory, .keep_all = TRUE) %>%
  filter(!is.na(cdo_year_established) & chief_data_officers >= 1) %>%
  transmute(state_territory, cdo_year_established)

earliest_cdo <- cdo_tbl %>%
  filter(cdo_year_established == min(cdo_year_established, na.rm = TRUE))
latest_cdo <- cdo_tbl %>%
  filter(cdo_year_established == max(cdo_year_established, na.rm = TRUE))

insights_cdo <- tibble(
  statement = c(
    glue("{earliest_cdo$state_territory} introduced CDOs earliest in {earliest_cdo$cdo_year_established}."),
    glue("{latest_cdo$state_territory} introduced CDOs the latest in {latest_cdo$cdo_year_established}.")
  )
)

# -------------------------------------------------------------------
# 4) DST analysis
dst_tbl <- dst_data_clean %>%
  filter(!is.na(team_year_founded)) %>%
  arrange(team_year_founded)

earliest_dst <- dst_tbl %>%
  filter(team_year_founded == min(team_year_founded, na.rm = TRUE))
latest_dst <- dst_tbl %>%
  filter(team_year_founded == max(team_year_founded, na.rm = TRUE))

insights_dst <- tibble(
  statement = c(
    glue("{earliest_dst$state_territory} introduced its Digital Service Team ({earliest_dst$team_name}) earliest in {earliest_dst$team_year_founded}."),
    glue("{latest_dst$state_territory} introduced its Digital Service Team ({latest_dst$team_name}) the latest in {latest_dst$team_year_founded}.")
  )
)

# -------------------------------------------------------------------
# 5) Save outputs
if (!dir.exists(here("output"))) dir.create(here("output"), recursive = TRUE)

write_csv(cdo_tbl,      here("output", "cdo_years_q4.csv"))
write_csv(insights_cdo, here("output", "insights_cdo_q4.csv"))

write_csv(dst_tbl,      here("output", "dst_years_q4.csv"))
write_csv(insights_dst, here("output", "insights_dst_q4.csv"))

# -------------------------------------------------------------------
# 6) Print in document
cdo_tbl
insights_cdo

dst_tbl
insights_dst
```

### Q5

-   Which states with **active CDOs** also have **design systems** or **DSTs**?

I examined whether states with CDOs also adopted other infrastructure (design systems or DSTs). After filtering to states with active CDOs, I tallied how many also had design systems or DSTs. I then compared the likelihood of having a DST between states with and without CDOs using a relative likelihood ratio.

```{r}
#| label: research-question-5
#| message: false
#| warning: false

library(readr); library(here)
library(dplyr); library(glue)

# -------------------------------------------------------------------
# 1) Load cleaned dataset
map_data_clean <- read_rds(here("output", "map_data_clean.rds"))

# -------------------------------------------------------------------
# 2) Prep: keep one row per state/territory, coerce presence to 0/1
q5_tbl <- map_data_clean %>%
  distinct(state_territory, .keep_all = TRUE) %>%
  transmute(
    state_territory,
    cdo    = as.integer(coalesce(chief_data_officers,        0L) >= 1L),
    design = as.integer(coalesce(design_systems,             0L) >= 1L),
    dst    = as.integer(coalesce(digital_service_teams,      0L) >= 1L)
  )

# -------------------------------------------------------------------
# 3) Which states with active CDOs also have design systems or DSTs?
cdo_active <- q5_tbl %>% filter(cdo == 1)

cdo_with_design_or_dst <- cdo_active %>%
  filter(design == 1 | dst == 1)

num_states <- nrow(cdo_with_design_or_dst)
state_names <- paste(cdo_with_design_or_dst$state_territory, collapse = ", ")

# -------------------------------------------------------------------
# 4) Likelihood comparison: probability of DST with CDO vs. without
p_dst_with_cdo <- mean(cdo_active$dst == 1, na.rm = TRUE)
p_dst_without  <- mean(q5_tbl %>% filter(cdo == 0) %>% pull(dst) == 1, na.rm = TRUE)

likelihood_ratio <- ifelse(p_dst_without > 0, p_dst_with_cdo / p_dst_without, NA_real_)

# Round ratio nicely (e.g., 2.2x more likely)
ratio_text <- ifelse(is.na(likelihood_ratio), "undefined",
                     paste0(round(likelihood_ratio, 1), "x more"))

# -------------------------------------------------------------------
# 5) Build insight statements
insights_q5 <- tibble(
  statement = c(
    glue("{num_states} states with active CDOs also have design systems or DSTs. They are {state_names}."),
    glue("States with Chief Data Officers are {ratio_text} likely to have digital service teams.")
  )
)

# -------------------------------------------------------------------
# 6) Save outputs
if (!dir.exists(here("output"))) dir.create(here("output"), recursive = TRUE)
write_csv(q5_tbl,        here("output", "state_counts_q5.csv"))
write_csv(insights_q5,   here("output", "insights_q5.csv"))

# -------------------------------------------------------------------
# 7) Print results
q5_tbl
insights_q5
```

### Q6

-   Are states with **AI legislation** more likely to have other forms of digital infrastructure (DST, design systems)?

I defined “digital infrastructure” broadly as the presence of any of the following: a CDO, DST, Digital Service Impact Report, or design system. I then compared the probability of having this infrastructure between states with versus without executive orders, AI legislation, or any policy (executive order or AI). Likelihood ratios were calculated to quantify differences, and notable examples were listed.

```{r}
#| label: research-question-6
#| message: false
#| warning: false

library(readr); library(here)
library(dplyr); library(glue)

# Load
map_data_clean <- read_rds(here("output", "map_data_clean.rds"))

# Prep (one row/state; presence 0/1)
q6_tbl <- map_data_clean %>%
  distinct(state_territory, .keep_all = TRUE) %>%
  transmute(
    state_territory,
    eo     = as.integer(coalesce(executive_orders,          0L) >= 1L),
    ai     = as.integer(coalesce(ai_legislations,           0L) >= 1L),
    cdo    = as.integer(coalesce(chief_data_officers,       0L) >= 1L),
    dst    = as.integer(coalesce(digital_service_teams,     0L) >= 1L),
    dsir   = as.integer(coalesce(digital_service_impact_reports, 0L) >= 1L),
    design = as.integer(coalesce(design_systems,            0L) >= 1L)
  ) %>%
  mutate(
    # DEFINITION NOTE:
    # "Digital infrastructure" = present if ANY of the following are present:
    #   CDO OR Digital Service Team (DST) OR Digital Service Impact Report (DSIR) OR Design system
    infra = as.integer(cdo == 1 | dst == 1 | dsir == 1 | design == 1),
    any_policy = as.integer(eo == 1 | ai == 1)
  )

# Helper: ratio + notable examples
calc_ratio <- function(df, policy_col) {
  with_policy    <- df %>% filter(.data[[policy_col]] == 1)
  without_policy <- df %>% filter(.data[[policy_col]] == 0)

  p_with    <- mean(with_policy$infra == 1, na.rm = TRUE)
  p_without <- mean(without_policy$infra == 1, na.rm = TRUE)

  ratio <- ifelse(p_without > 0, p_with / p_without, NA_real_)
  ratio_text <- ifelse(is.na(ratio), "undefined", paste0(round(ratio, 1), "x more"))

  notable <- with_policy %>% filter(infra == 1) %>% pull(state_territory)
  notable_text <- ifelse(length(notable) > 0, paste(notable, collapse = ", "), "None")

  list(ratio_text = ratio_text, notable_text = notable_text)
}

# Ratios + statements
eo_stats  <- calc_ratio(q6_tbl, "eo")
ai_stats  <- calc_ratio(q6_tbl, "ai")
any_stats <- calc_ratio(q6_tbl, "any_policy")

insights_q6 <- tibble(
  statement = c(
    glue("States with executive orders are {eo_stats$ratio_text} likely to have other forms of digital infrastructure (CDO, DST, Digital Service Impact Report, design system). Notable examples are {eo_stats$notable_text}."),
    glue("States with AI legislation are {ai_stats$ratio_text} likely to have other forms of digital infrastructure (CDO, DST, Digital Service Impact Report, design system). Notable examples are {ai_stats$notable_text}."),
    glue("States with any policy (AI legislation and/or executive orders) are {any_stats$ratio_text} likely to have other forms of digital infrastructure (CDO, DST, Digital Service Impact Report, design system). Notable examples are {any_stats$notable_text}.")
  )
)

# Save outputs
if (!dir.exists(here("output"))) dir.create(here("output"), recursive = TRUE)
write_csv(q6_tbl,      here("output", "state_counts_q6.csv"))
write_csv(insights_q6, here("output", "insights_q6.csv"))

# Print results
q6_tbl
insights_q6
```

### Q7

-   In which region(s) do **AI legislation policies cluster** in?

To assess whether AI legislation clusters in particular regions, I calculated both **absolute counts** and **relative proportions**. First, I assigned each state or territory to its CDC region grouping and flagged whether it had AI legislation. I then aggregated by region to count the number of states with AI legislation (absolute clustering) and the percentage of states within the region that had AI legislation (relative clustering). Absolute counts highlight where most AI laws are located overall, while proportions account for differences in region size.

```{r}
#| label: research-question-7
#| message: false
#| warning: false

library(readr); library(here)
library(dplyr); library(glue); library(tidyr)

# 1) Load
map_data_clean <- read_rds(here("output", "map_data_clean.rds"))

# 2) Prep: one row per state/territory with region + AI presence (0/1)
q7_tbl <- map_data_clean %>%
  distinct(state_territory, .keep_all = TRUE) %>%
  transmute(
    state_territory,
    region = as.character(region),
    ai_legislation = as.integer(dplyr::coalesce(ai_legislations, 0L) >= 1L)
  )

# 3) Summarize by region: counts and proportions
region_ai_counts <- q7_tbl %>%
  group_by(region) %>%
  summarise(
    n_states       = n(),
    ai_states      = sum(ai_legislation, na.rm = TRUE),
    percent_with_ai= round(100 * ai_states / n_states, 1),
    .groups = "drop"
  ) %>%
  arrange(desc(ai_states), desc(percent_with_ai))

# 4) Identify clustering by absolute count and by proportion (tie-aware)
max_count <- max(region_ai_counts$ai_states, na.rm = TRUE)
max_prop  <- max(region_ai_counts$percent_with_ai, na.rm = TRUE)

cluster_count_regions <- region_ai_counts %>%
  filter(ai_states == max_count) %>%
  pull(region)

cluster_prop_regions <- region_ai_counts %>%
  filter(percent_with_ai == max_prop) %>%
  pull(region)

fmt_regions <- function(x) paste(x, collapse = ", ")

# 5) Insight statements (both views)
insights_q7 <- tibble(
  statement = c(
    glue("By absolute count, AI legislation tends to cluster in the {fmt_regions(cluster_count_regions)}."),
    glue("By share of states, AI legislation tends to cluster in the {fmt_regions(cluster_prop_regions)}.")
  )
)

# 6) Save
if (!dir.exists(here("output"))) dir.create(here("output"), recursive = TRUE)
write_csv(region_ai_counts, here("output", "region_ai_counts_q7.csv"))
write_csv(insights_q7,      here("output", "insights_q7.csv"))

# 7) Print
region_ai_counts
insights_q7
```

### Q8

-   What are the **most common policy topics** in executive orders (e.g., broadband, interoperability, AI)?

I parsed `executive_order_last_topic_tag_s` by normalizing separators (commas/semicolons → semicolons), splitting multi-topic strings into individual rows, trimming whitespace, and standardizing to title case. I then counted topic frequencies across all executive orders to identify the most and least common topics. For the word cloud, I used the `wordcloud` package with a fixed seed for reproducibility, scaling word size by frequency and using a categorical palette (no stemming or stop-word removal, because these are curated tags rather than free text).

```{r}
#| label: research-question-8
#| message: false
#| warning: false

library(readr); library(here)
library(dplyr); library(tidyr); library(stringr); library(glue)
library(wordcloud); library(RColorBrewer)

# 1) Load
policy_data_clean <- read_rds(here("output", "policy_data_clean.rds"))

# 2) Extract + clean EO topics (split multi-tags, normalize separators/spaces/case)
eo_topics <- policy_data_clean %>%
  filter(!is.na(executive_order_last_topic_tag_s)) %>%
  mutate(topic_str = str_replace_all(executive_order_last_topic_tag_s, "[,|;]", ";")) %>%
  separate_rows(topic_str, sep = ";") %>%
  mutate(topic = str_squish(str_to_title(topic_str))) %>%
  filter(topic != "")

# 3) Count topic frequency
topic_counts <- eo_topics %>%
  count(topic, sort = TRUE, name = "count")

# 4) Top/Bottom 10 (ties kept)
top10_topics    <- topic_counts %>% slice_max(count, n = 10, with_ties = TRUE)
bottom10_topics <- topic_counts %>% slice_min(count, n = 10, with_ties = TRUE)

insights_q8 <- tibble(
  statement = c(
    glue("The 10 most common policy topics in executive orders are: {paste(top10_topics$topic, collapse = ', ')}."),
    glue("The 10 least common policy topics in executive orders are: {paste(bottom10_topics$topic, collapse = ', ')}.")
  )
)

# 5) Save tables
if (!dir.exists(here("output"))) dir.create(here("output"), recursive = TRUE)
write_csv(topic_counts,    here("output", "eo_topic_counts_q8.csv"))
write_csv(insights_q8,     here("output", "insights_q8.csv"))
write_csv(top10_topics,    here("output", "eo_top10_topics_q8.csv"))
write_csv(bottom10_topics, here("output", "eo_bottom10_topics_q8.csv"))

# 6) Word cloud PNG (deterministic)
set.seed(123)
png(filename = here("output", "eo_topics_wordcloud_q8.png"),
    width = 1400, height = 1000, res = 150, bg = "white")
suppressWarnings(
  wordcloud(words = topic_counts$topic,
            freq  = topic_counts$count,
            min.freq = 1,
            max.words = 200,
            random.order = FALSE,
            rot.per = 0.10,
            scale = c(4, 0.8),
            colors = brewer.pal(8, "Dark2"))
)
dev.off()

# 7) Print in document
topic_counts
insights_q8
```

### Q9

-   What are the **most common policy topics** in AI legislations orders (e.g., broadband, interoperability, AI)?

I analyzed policy topics in AI legislations using the `bill_topic_tag_s_text` field. I normalized multi-topic strings (commas/semicolons → semicolons), split into individual tags, trimmed for whitespace, and standardized to title case. Because every law in this dataset is inherently about artificial intelligence, I excluded the term *“Artificial Intelligence”* to avoid inflating its frequency. Topics were then tallied across all AI legislations to identify the most and least common. For visualization, I generated a word cloud with word size scaled by topic frequency and colors drawn from a categorical palette. A fixed random seed ensures reproducibility.

```{r}
#| label: research-question-9
#| message: false
#| warning: false

library(readr); library(here)
library(dplyr); library(tidyr); library(stringr); library(glue)
library(wordcloud); library(RColorBrewer)

# 1) Load
policy_data_clean <- read_rds(here("output", "policy_data_clean.rds"))

# 2) Extract + clean AI legislation topics
# Variable of interest: bill_topic_tag_s_text (multi-value tags)
ai_topics <- policy_data_clean %>%
  filter(!is.na(bill_topic_tag_s_text)) %>%
  mutate(topic_str = str_replace_all(bill_topic_tag_s_text, "[,|;]", ";")) %>%
  separate_rows(topic_str, sep = ";") %>%
  mutate(topic = str_squish(str_to_title(topic_str))) %>%
  filter(topic != "") %>%
  filter(topic != "Artificial Intelligence")   # remove base term to avoid inflation

# 3) Count frequency
ai_topic_counts <- ai_topics %>%
  count(topic, sort = TRUE, name = "count")

# 4) Top/Bottom 10 (ties kept)
top10_ai_topics    <- ai_topic_counts %>% slice_max(count, n = 10, with_ties = TRUE)
bottom10_ai_topics <- ai_topic_counts %>% slice_min(count, n = 10, with_ties = TRUE)

insights_q9 <- tibble(
  statement = c(
    glue("The 10 most common policy topics in AI legislations are: {paste(top10_ai_topics$topic, collapse = ', ')}."),
    glue("The 10 least common policy topics in AI legislations are: {paste(bottom10_ai_topics$topic, collapse = ', ')}.")
  )
)

# 5) Save tables
if (!dir.exists(here("output"))) dir.create(here("output"), recursive = TRUE)
write_csv(ai_topic_counts,    here("output", "ai_topic_counts_q9.csv"))
write_csv(insights_q9,        here("output", "insights_q9.csv"))
write_csv(top10_ai_topics,    here("output", "ai_top10_topics_q9.csv"))
write_csv(bottom10_ai_topics, here("output", "ai_bottom10_topics_q9.csv"))

# 6) Word cloud PNG (deterministic)
set.seed(123)
png(filename = here("output", "ai_topics_wordcloud_q9.png"),
    width = 1400, height = 1000, res = 150, bg = "white")
suppressWarnings(
  wordcloud(words = ai_topic_counts$topic,
            freq  = ai_topic_counts$count,
            min.freq = 1,
            max.words = 200,
            random.order = FALSE,
            rot.per = 0.10,
            scale = c(4, 0.8),
            colors = brewer.pal(8, "Dark2"))
)
dev.off()

# 7) Print in document
ai_topic_counts
insights_q9
```

### Q10

-   What is the **trend over time** in the number of executive orders and AI legislations enacted?

I examined temporal trends by counting the number of executive orders and AI legislations enacted each year. For executive orders, I used the `executive_order_year_enacted` variable; for AI legislations, I used the `legislative_session` variable. Counts were aggregated by year, with missing years filled as zeros to provide a continuous timeline. I then described whether activity has generally increased, decreased, or remained the same by looking at the overall slope of that line (positive/negative/flat) for the overall pattern of counts across time.

```{r}
#| label: research-question-10
#| message: false
#| warning: false

library(readr); library(here)
library(dplyr); library(tidyr); library(stringr); library(glue)

# 1) Load
policy_data_clean <- read_rds(here("output", "policy_data_clean.rds"))

# 2) Normalize scan_type and build yearly counts for both series
dat <- policy_data_clean %>%
  mutate(scan_type_norm = str_to_lower(str_squish(scan_type)))

eo_by_year <- dat %>%
  filter(str_detect(scan_type_norm, "executive"),
         !is.na(executive_order_year_enacted)) %>%
  transmute(year = as.integer(executive_order_year_enacted)) %>%
  count(year, name = "count") %>%
  mutate(type = "Executive orders")

ai_by_year <- dat %>%
  filter(str_detect(scan_type_norm, "legis"),
         !is.na(legislative_session)) %>%
  transmute(year = as.integer(legislative_session)) %>%
  count(year, name = "count") %>%
  mutate(type = "AI legislations")

# 3) Fill missing years with zero so both series span the same axis
all_years <- sort(unique(c(eo_by_year$year, ai_by_year$year)))
trend_wide <- tibble(year = all_years) %>%
  left_join(select(eo_by_year, year, eo = count), by = "year") %>%
  left_join(select(ai_by_year, year, ai = count), by = "year") %>%
  mutate(across(c(eo, ai), ~ replace_na(., 0L)))

trend_long <- trend_wide %>%
  pivot_longer(c(eo, ai), names_to = "series", values_to = "count") %>%
  mutate(type = recode(series, eo = "Executive orders", ai = "AI legislations")) %>%
  select(year, type, count) %>%
  arrange(year, type)

# 4) Direction statements via simple linear trend on filled series
slope_dir <- function(x_year, y_count) {
  if (length(unique(x_year)) < 2) return("remained the same")
  s <- coef(lm(y_count ~ x_year))[2]
  if (is.na(s) || abs(s) < 1e-9) "remained the same" else if (s > 0) "increased" else "decreased"
}
eo_dir <- slope_dir(trend_wide$year, trend_wide$eo)
ai_dir <- slope_dir(trend_wide$year, trend_wide$ai)

insights_q10 <- tibble(
  statement = c(
    glue("On average, the number of executive orders around digital transformation in government have {eo_dir} over the years."),
    glue("On average, the number of AI legislations around digital transformation in government have {ai_dir} over the years.")
  )
)

# 5) Save + print
if (!dir.exists(here("output"))) dir.create(here("output"), recursive = TRUE)
write_csv(trend_long,   here("output", "trend_data_q10.csv"))
write_csv(insights_q10, here("output", "insights_q10.csv"))

trend_long
insights_q10
```

### Q11

-   Do certain AI legislation topics **cluster in certain regions**? Do certain executive topics topics cluster in certain regions?

For executive orders, I used the `executive_order_last_topic_tag_s` variable, and for AI legislation, I used the `bill_topic_tag_s_text` variable. Multi-topic strings were split on commas and semicolons, trimmed, and normalized. For AI legislation, I removed the generic tag “artificial intelligence” since the dataset itself is scoped to AI-related laws, and including it would artificially inflate its frequency. I then counted topic mentions by region and reported the top five most common executive order topics and top five AI legislation topics for each region , with word size proportional to topic frequency. The results are reported as paired statements by region.

```{r}
#| label: research-question-11
#| message: false
#| warning: false

library(readr); library(here)
library(dplyr); library(tidyr); library(stringr); library(glue)
library(purrr)
library(wordcloud); library(RColorBrewer)

# 1) Load
policy_data_clean <- read_rds(here("output", "policy_data_clean.rds"))

# 2) Helper: safe filename
slug <- function(x) {
  x |> tolower() |> str_replace_all("[^a-z0-9]+", "_") |> str_replace_all("^_|_$", "")
}

# 3) Executive order topics by region (split inline; no map/list-cols)
eo_topics_by_region <- policy_data_clean %>%
  filter(!is.na(region), !is.na(executive_order_last_topic_tag_s)) %>%
  transmute(region,
            topic = executive_order_last_topic_tag_s) %>%
  separate_rows(topic, sep = "[,;]") %>%
  mutate(topic = str_squish(str_to_title(topic))) %>%
  filter(topic != "") %>%
  count(region, topic, sort = TRUE)

# 4) AI legislation topics by region (exclude "Artificial Intelligence")
ai_topics_by_region <- policy_data_clean %>%
  filter(!is.na(region), !is.na(bill_topic_tag_s_text)) %>%
  transmute(region,
            topic = bill_topic_tag_s_text) %>%
  separate_rows(topic, sep = "[,;]") %>%
  mutate(topic = str_squish(str_to_title(topic))) %>%
  filter(topic != "", topic != "Artificial Intelligence") %>%
  count(region, topic, sort = TRUE)

# 5) Top 5 per region (EO & AI)
eo_top5 <- eo_topics_by_region %>%
  group_by(region) %>%
  slice_max(n, n = 5, with_ties = FALSE) %>%
  summarise(eo_top5 = paste(topic, collapse = ", "), .groups = "drop")

ai_top5 <- ai_topics_by_region %>%
  group_by(region) %>%
  slice_max(n, n = 5, with_ties = FALSE) %>%
  summarise(ai_top5 = paste(topic, collapse = ", "), .groups = "drop")

region_topics_q11 <- eo_top5 %>%
  full_join(ai_top5, by = "region") %>%
  arrange(region)

# 6) Statements (one per region, exactly as requested)
insights_q11 <- region_topics_q11 %>%
  mutate(statement = glue(
    "The top five most common executive order topics in the {region} are {eo_top5}, ",
    "while {ai_top5} are the top five most common."
  )) %>%
  select(region, statement)

# 7) Save tables
if (!dir.exists(here("output"))) dir.create(here("output"), recursive = TRUE)
write_csv(eo_topics_by_region, here("output", "eo_topics_by_region_q11.csv"))
write_csv(ai_topics_by_region, here("output", "ai_topics_by_region_q11.csv"))
write_csv(region_topics_q11,   here("output", "region_topics_q11.csv"))
write_csv(insights_q11,        here("output", "insights_q11.csv"))

# 8) Regional word clouds (EO and AI); deterministic with set.seed
set.seed(123)
unique_regions <- sort(unique(c(eo_topics_by_region$region, ai_topics_by_region$region)))

walk(unique_regions, function(reg) {
  # EO word cloud for region (if data exists)
  eo_reg <- eo_topics_by_region %>% filter(region == reg)
  if (nrow(eo_reg) > 0) {
    png(filename = here("output", paste0("wordcloud_eo_", slug(reg), "_q11.png")),
        width = 1400, height = 1000, res = 150, bg = "white")
    suppressWarnings(
      wordcloud(words = eo_reg$topic,
                freq  = eo_reg$n,
                min.freq = 1,
                max.words = 200,
                random.order = FALSE,
                rot.per = 0.10,
                scale = c(4, 0.8),
                colors = brewer.pal(8, "Dark2"))
    )
    dev.off()
  }

  # AI word cloud for region (if data exists)
  ai_reg <- ai_topics_by_region %>% filter(region == reg)
  if (nrow(ai_reg) > 0) {
    png(filename = here("output", paste0("wordcloud_ai_", slug(reg), "_q11.png")),
        width = 1400, height = 1000, res = 150, bg = "white")
    suppressWarnings(
      wordcloud(words = ai_reg$topic,
                freq  = ai_reg$n,
                min.freq = 1,
                max.words = 200,
                random.order = FALSE,
                rot.per = 0.10,
                scale = c(4, 0.8),
                colors = brewer.pal(8, "Dark2"))
    )
    dev.off()
  }
})

# 9) Print in document
region_topics_q11
insights_q11
```

### Q12

-   How do states that first adopted **(early adopters)** digital government executive order policies between **2013–2015** differ from those **(late adopters)** that first adopted them between **2023–2025** in terms of:

    -   Policy topics (executive order topic tags)

    -   Regional distribution (region 1–5 categories)

I compared early adopters of digital government executive orders (2013–2015) to late adopters (2023–2025). First, I grouped states into early or late cohorts using the `executive_order_year_enacted` field. Multi-topic strings in `executive_order_last_topic_tag_s` were split on commas/semicolons, trimmed, and standardized to title case. I summarized the top five topics for each group and visualized them with word clouds. Regional distributions were assessed by counting the number of executive orders per region for early vs. late adopters. Given the limited sample sizes and topic sparsity, I focused on descriptive comparisons only rather than formal statistical testing.

```{r}
#| label: research-question-12
#| message: false
#| warning: false

library(readr); library(here)
library(dplyr); library(tidyr); library(stringr); library(glue)
library(wordcloud); library(RColorBrewer)

# 1) Load data
policy_data_clean <- read_rds(here("output", "policy_data_clean.rds"))

# 2) Subset executive orders with years
eo <- policy_data_clean %>%
  filter(!is.na(executive_order_year_enacted),
         !is.na(executive_order_last_topic_tag_s),
         executive_order_year_enacted >= 2013,
         executive_order_year_enacted <= 2025) %>%
  select(state_territory, region, executive_order_year_enacted,
         executive_order_last_topic_tag_s)

# 3) Group into early vs late adopters
eo <- eo %>%
  mutate(period = case_when(
    between(executive_order_year_enacted, 2013, 2015) ~ "Early (2013–2015)",
    between(executive_order_year_enacted, 2023, 2025) ~ "Late (2023–2025)",
    TRUE ~ NA_character_
  )) %>%
  filter(!is.na(period))

# 4) Split topics
eo_topics <- eo %>%
  separate_rows(executive_order_last_topic_tag_s, sep = "[,;]") %>%
  mutate(topic = str_squish(str_to_title(executive_order_last_topic_tag_s))) %>%
  filter(topic != "")

# 5) Frequency tables
topic_counts <- eo_topics %>%
  count(period, topic, sort = TRUE)

top5_early <- topic_counts %>%
  filter(period == "Early (2013–2015)") %>%
  slice_max(n, n = 5, with_ties = FALSE)

top5_late <- topic_counts %>%
  filter(period == "Late (2023–2025)") %>%
  slice_max(n, n = 5, with_ties = FALSE)

# 6) Regional distribution (counts per region)
region_counts_q12 <- eo %>%
  count(period, region, name = "count")

# 7) Statements (purely descriptive)
insights_q12 <- tibble(
  statement = c(
    glue("The top five most common executive order topics among early adopters (2013–2015) were {paste(top5_early$topic, collapse = ', ')}, while late adopters (2023–2025) focused more on {paste(top5_late$topic, collapse = ', ')}."),
    glue("Regionally, early adopters were concentrated in: {paste(unique(region_counts_q12$region[region_counts_q12$period == 'Early (2013–2015)']), collapse = ', ')}; late adopters in: {paste(unique(region_counts_q12$region[region_counts_q12$period == 'Late (2023–2025)']), collapse = ', ')}.")
  )
)

# 8) Save outputs
if (!dir.exists(here("output"))) dir.create(here("output"), recursive = TRUE)
write_csv(topic_counts,      here("output", "eo_topic_counts_q12.csv"))
write_csv(region_counts_q12, here("output", "region_counts_q12.csv"))
write_csv(insights_q12,      here("output", "insights_q12.csv"))

# 9) Word clouds (early vs late)
set.seed(123)
for (grp in unique(eo_topics$period)) {
  dat <- eo_topics %>% filter(period == grp) %>% count(topic)
  if (nrow(dat) > 0) {
    png(filename = here("output", paste0("wordcloud_", str_replace_all(grp, "[^A-Za-z0-9]", "_"), "_q12.png")),
        width = 1400, height = 1000, res = 150, bg = "white")
    wordcloud(words = dat$topic,
              freq = dat$n,
              min.freq = 1,
              max.words = 200,
              random.order = FALSE,
              rot.per = 0.10,
              scale = c(4, 0.8),
              colors = brewer.pal(8, "Dark2"))
    dev.off()
  }
}

# 10) Print
top5_early
top5_late
region_counts_q12
insights_q12
```

### Q13

-   How do states that first adopted **(early adopters)** AI legislation between **2019–2021** differ from those **(late adopters)** that first adopted it between **2023–2025** in terms of:

    -   Policy topics (bill topic tags, excluding the baseline “artificial intelligence” tag)

    -   Regional distribution (region 1–5 categories)

I compared early adopters of AI legislation (2019–2021) to late adopters (2023–2025). I grouped states by adoption period using the `legislative_session` field. Policy topics (`bill_topic_tag_s_text`) were split on commas/semicolons, cleaned, and standardized to title case. Because all bills in this dataset are inherently AI-related, I excluded the generic tag *“Artificial Intelligence”* to avoid inflating its frequency and to highlight the other substantive policy areas addressed in AI legislation (e.g., data governance, privacy, workforce). I then summarized the top five topics for each group, created word clouds for visualization, and compared regional distributions by counting the number of adopting states in each CDC region.

```{r}
#| label: research-question-13
#| message: false
#| warning: false

library(readr); library(here)
library(dplyr); library(tidyr); library(stringr); library(glue)
library(wordcloud); library(RColorBrewer)

# 1) Load data
policy_data_clean <- read_rds(here("output", "policy_data_clean.rds"))

# 2) Subset AI legislations with years
ai <- policy_data_clean %>%
  filter(!is.na(legislative_session),
         !is.na(bill_topic_tag_s_text),
         legislative_session >= 2019,
         legislative_session <= 2025) %>%
  select(state_territory, region, legislative_session,
         bill_topic_tag_s_text)

# 3) Group into early vs late adopters
ai <- ai %>%
  mutate(period = case_when(
    between(legislative_session, 2019, 2021) ~ "Early (2019–2021)",
    between(legislative_session, 2023, 2025) ~ "Late (2023–2025)",
    TRUE ~ NA_character_
  )) %>%
  filter(!is.na(period))

# 4) Split topics, clean, and exclude "Artificial Intelligence"
ai_topics <- ai %>%
  separate_rows(bill_topic_tag_s_text, sep = "[,;]") %>%
  mutate(topic = str_squish(str_to_title(bill_topic_tag_s_text))) %>%
  filter(topic != "", topic != "Artificial Intelligence")

# 5) Frequency tables
topic_counts <- ai_topics %>%
  count(period, topic, sort = TRUE)

top5_early <- topic_counts %>%
  filter(period == "Early (2019–2021)") %>%
  slice_max(n, n = 5, with_ties = FALSE)

top5_late <- topic_counts %>%
  filter(period == "Late (2023–2025)") %>%
  slice_max(n, n = 5, with_ties = FALSE)

# 6) Regional distribution (counts per region)
region_counts_q13 <- ai %>%
  count(period, region, name = "count")

# 7) Statements
insights_q13 <- tibble(
  statement = c(
    glue("Early adopters of AI legislation (2019–2021) were concentrated in regions {paste(unique(region_counts_q13$region[region_counts_q13$period == 'Early (2019–2021)']), collapse = ', ')}, while late adopters (2023–2025) were more evenly spread across regions {paste(unique(region_counts_q13$region[region_counts_q13$period == 'Late (2023–2025)']), collapse = ', ')}."),
    glue("The top AI legislation topics for early adopters were {paste(top5_early$topic, collapse = ', ')}, while late adopters more commonly emphasized {paste(top5_late$topic, collapse = ', ')}.")
  )
)

# 8) Save outputs
if (!dir.exists(here("output"))) dir.create(here("output"), recursive = TRUE)
write_csv(topic_counts,      here("output", "ai_topic_counts_q13.csv"))
write_csv(region_counts_q13, here("output", "region_counts_q13.csv"))
write_csv(insights_q13,      here("output", "insights_q13.csv"))

# 9) Word clouds (early vs late)
set.seed(123)
for (grp in unique(ai_topics$period)) {
  dat <- ai_topics %>% filter(period == grp) %>% count(topic)
  if (nrow(dat) > 0) {
    png(filename = here("output", paste0("wordcloud_", str_replace_all(grp, "[^A-Za-z0-9]", "_"), "_q13.png")),
        width = 1400, height = 1000, res = 150, bg = "white")
    wordcloud(words = dat$topic,
              freq = dat$n,
              min.freq = 1,
              max.words = 200,
              random.order = FALSE,
              rot.per = 0.10,
              scale = c(4, 0.8),
              colors = brewer.pal(8, "Dark2"))
    dev.off()
  }
}

# 10) Print
top5_early
top5_late
region_counts_q13
insights_q13
```

## Export all insights

I'm almost done! I now merge all insights/statements into one document to use for our reporting.

```{r}
#| label: collect-all-insights
#| message: false
#| warning: false

library(tidyverse)
library(here)
library(fs)

# 1) Files to include (ordered) — keep Q4 split files; exclude insights_q4.csv
insight_files <- c(
  "insights.csv",                # Q1
  "insights_q2.csv",
  "insights_q3.csv",
  "insights_cdo_q4.csv",         # Q4 (part 1)
  "insights_dst_q4.csv",         # Q4 (part 2)
  "insights_q5.csv",
  "insights_q6.csv",
  "insights_q7.csv",
  "insights_q8.csv",
  "insights_q9.csv",
  "insights_q10.csv",
  "insights_q11.csv",
  "insights_q12.csv",
  "insights_q13.csv"
)

# 2) Map filenames -> question numbers
q_map <- c(
  "insights.csv"       = "Q1",
  "insights_q2.csv"    = "Q2",
  "insights_q3.csv"    = "Q3",
  "insights_cdo_q4.csv"= "Q4",
  "insights_dst_q4.csv"= "Q4",
  "insights_q5.csv"    = "Q5",
  "insights_q6.csv"    = "Q6",
  "insights_q7.csv"    = "Q7",
  "insights_q8.csv"    = "Q8",
  "insights_q9.csv"    = "Q9",
  "insights_q10.csv"   = "Q10",
  "insights_q11.csv"   = "Q11",
  "insights_q12.csv"   = "Q12",
  "insights_q13.csv"   = "Q13"
)

# 3) Map question numbers -> full research question text
q_text_map <- c(
  Q1  = "How many states/territories have at least one: Executive order, AI Legislation, Chief Data Officer (CDO), Digital Service Team (DST), Digital Service Impact Report, Design system",
  Q2  = "Which regions have the most vs least: Executive order, AI Legislations, Chief Data Officers (CDO), Digital Service Teams (DST), Digital Service Impact Reports, Design systems",
  Q3  = "Which states are the most active vs. least active across all categories?",
  Q4  = "Which states introduced CDOs or DSTs earliest, and how has that spread across regions?",
  Q5  = "Which states with active CDOs also have design systems or DSTs?",
  Q6  = "Are states with AI legislation more likely to have other forms of digital infrastructure (DST, design systems)?",
  Q7  = "In which region(s) do AI legislation policies cluster in?",
  Q8  = "What are the most common policy topics in executive orders (e.g., broadband, interoperability, AI)?",
  Q9  = "What are the most common policy topics in AI legislations orders (e.g., broadband, interoperability, AI)?",
  Q10 = "What is the trend over time in the number of executive orders and AI legislations enacted?",
  Q11 = "Do certain AI legislation topics cluster in certain regions? Do certain executive topics topics cluster in certain regions?",
  Q12 = "How do states that first adopted (early adopters) digital government executive order policies between 2013–2015 differ from those (late adopters) that first adopted them between 2023–2025 in terms of: Policy topics (executive order topic tags) and Regional distribution (region 1–5 categories)",
  Q13 = "How do states that first adopted (early adopters) AI legislation between 2019–2021 differ from those (late adopters) that first adopted it between 2023–2025 in terms of: Policy topics (bill topic tags, excluding the baseline 'artificial intelligence' tag) and Regional distribution (region 1–5 categories)"
)

# 4) Keep only files that actually exist in output/
insight_files <- keep(insight_files, ~ file_exists(here("output", .x)))

# 5) Reader with special handling for Q3 ("type" + "statement")
safe_read <- function(fname) {
  path <- here("output", fname)
  df <- readr::read_csv(path, show_col_types = FALSE)

  if ("statement" %in% names(df)) {
    if ("type" %in% names(df)) {
      df <- df %>%
        mutate(statement = paste0(type, ": ", statement)) %>%
        select(statement)
    } else {
      df <- df %>% select(statement)
    }
  } else {
    return(tibble())
  }

  qnum <- q_map[[fname]]
  df %>%
    mutate(
      source_file = fname,
      question_number = qnum,
      question_text = q_text_map[[qnum]]
    ) %>%
    select(question_number, question_text, source_file, statement)
}

# 6) Bind, keep input order
all_insights_statements <- map_dfr(insight_files, safe_read) %>%
  mutate(
    question_number = factor(
      question_number,
      levels = unique(q_map[insight_files])  # preserve order
    )
  ) %>%
  arrange(question_number)

# 7) Save
readr::write_csv(all_insights_statements, here("output", "all_insights_statements.csv"))
```
